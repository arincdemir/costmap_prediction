{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from CNMP import CNMP \n",
    "from dataset import GridDataset \n",
    "import wandb\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marinc-demir\u001b[0m (\u001b[33marinc-demir-bo-azi-i-niversitesi\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arinc/workspace/costmap_prediction/Conv_CNMP/wandb/run-20250224_184526-en2jkjn9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/arinc-demir-bo-azi-i-niversitesi/ped_forecasting/runs/en2jkjn9' target=\"_blank\">woven-cosmos-94</a></strong> to <a href='https://wandb.ai/arinc-demir-bo-azi-i-niversitesi/ped_forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/arinc-demir-bo-azi-i-niversitesi/ped_forecasting' target=\"_blank\">https://wandb.ai/arinc-demir-bo-azi-i-niversitesi/ped_forecasting</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/arinc-demir-bo-azi-i-niversitesi/ped_forecasting/runs/en2jkjn9' target=\"_blank\">https://wandb.ai/arinc-demir-bo-azi-i-niversitesi/ped_forecasting/runs/en2jkjn9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/arinc-demir-bo-azi-i-niversitesi/ped_forecasting/runs/en2jkjn9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f98f6ac9580>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "t_dim = 1                   # step index dimension\n",
    "SM_dim = 32 ** 2            # grid flattened dimension (16*16)\n",
    "encoder_hidden_dims = [1024, 1024, 1024]\n",
    "latent_dim = 1024\n",
    "decoder_hidden_dims = [1024, 1024, 1024]\n",
    "\n",
    "batch_size = 15\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "wandb.init(\n",
    "    project=\"ped_forecasting\",\n",
    "    config={\"learning_rate\": 0.0001,\n",
    "            \"architecture\": \"CNMP\",\n",
    "            \"epochs\": num_epochs,\n",
    "            \"encoder_hidden_dims\": encoder_hidden_dims,\n",
    "            \"latent_dim\": latent_dim,\n",
    "            \"decoder_hidden_dims\": decoder_hidden_dims}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9892/490913078.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  grids_tensor = torch.load(data_path)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Load data generated earlier\n",
    "data_path = \"grids_tensor.pt\"\n",
    "grids_tensor = torch.load(data_path)\n",
    "\n",
    "# Create the dataset and split into train and validation sets\n",
    "dataset = GridDataset(grids_tensor, max_encodings=10, max_queries=10)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset = torch.utils.data.Subset(dataset, range(val_size, len(dataset)))\n",
    "val_dataset = torch.utils.data.Subset(dataset, range(val_size))\n",
    "\n",
    "# TODO I did this for trying to overfit the model with just one simulation.\n",
    "#train_dataset = dataset\n",
    "#val_dataset = dataset\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the model and optimizer\n",
    "model = CNMP(\n",
    "    t_dim=t_dim,\n",
    "    SM_dim=SM_dim,\n",
    "    encoder_hidden_dims=encoder_hidden_dims,\n",
    "    decoder_hidden_dims=decoder_hidden_dims,\n",
    "    latent_dim=latent_dim\n",
    ").to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Training Loss: 0.1011 Time Left: 529.25 seconds\n",
      "Epoch 2/1000, Training Loss: 0.1001 Time Left: 282.06 seconds\n",
      "Epoch 3/1000, Training Loss: 0.0817 Time Left: 197.90 seconds\n",
      "Epoch 4/1000, Training Loss: 0.0918 Time Left: 156.22 seconds\n",
      "Epoch 5/1000, Training Loss: 0.0797 Time Left: 131.90 seconds\n",
      "Epoch 6/1000, Training Loss: 0.0888 Time Left: 114.79 seconds\n",
      "Epoch 7/1000, Training Loss: 0.0906 Time Left: 102.60 seconds\n",
      "Epoch 8/1000, Training Loss: 0.0939 Time Left: 93.47 seconds\n",
      "Epoch 9/1000, Training Loss: 0.0936 Time Left: 86.32 seconds\n",
      "Epoch 10/1000, Training Loss: 0.0649 Time Left: 80.66 seconds\n",
      "Epoch 11/1000, Training Loss: 0.1985 Time Left: 75.67 seconds\n",
      "Epoch 12/1000, Training Loss: 0.0931 Time Left: 71.49 seconds\n",
      "Epoch 13/1000, Training Loss: 0.0959 Time Left: 67.88 seconds\n",
      "Epoch 14/1000, Training Loss: 0.0959 Time Left: 64.70 seconds\n",
      "Epoch 15/1000, Training Loss: 0.0949 Time Left: 62.02 seconds\n",
      "Epoch 16/1000, Training Loss: 0.0930 Time Left: 59.49 seconds\n",
      "Epoch 17/1000, Training Loss: 0.0881 Time Left: 57.39 seconds\n",
      "Epoch 18/1000, Training Loss: 0.0694 Time Left: 55.41 seconds\n",
      "Epoch 19/1000, Training Loss: 0.1050 Time Left: 53.64 seconds\n",
      "Epoch 20/1000, Training Loss: 0.0515 Time Left: 52.02 seconds\n",
      "Epoch 21/1000, Training Loss: 0.0690 Time Left: 50.56 seconds\n",
      "Epoch 22/1000, Training Loss: 0.0778 Time Left: 49.24 seconds\n",
      "Epoch 23/1000, Training Loss: 0.0767 Time Left: 48.12 seconds\n",
      "Epoch 24/1000, Training Loss: 0.0625 Time Left: 46.99 seconds\n",
      "Epoch 25/1000, Training Loss: 0.0498 Time Left: 46.01 seconds\n",
      "Epoch 26/1000, Training Loss: 0.4922 Time Left: 45.10 seconds\n",
      "Epoch 27/1000, Training Loss: 0.0756 Time Left: 44.28 seconds\n",
      "Epoch 28/1000, Training Loss: 0.0850 Time Left: 43.45 seconds\n",
      "Epoch 29/1000, Training Loss: 0.0854 Time Left: 42.64 seconds\n",
      "Epoch 30/1000, Training Loss: 0.0837 Time Left: 41.96 seconds\n",
      "Epoch 31/1000, Training Loss: 0.0788 Time Left: 41.28 seconds\n",
      "Epoch 32/1000, Training Loss: 0.0691 Time Left: 40.65 seconds\n",
      "Epoch 33/1000, Training Loss: 0.0507 Time Left: 40.04 seconds\n",
      "Epoch 34/1000, Training Loss: 0.0406 Time Left: 39.52 seconds\n",
      "Epoch 35/1000, Training Loss: 0.0497 Time Left: 39.04 seconds\n",
      "Epoch 36/1000, Training Loss: 0.0430 Time Left: 38.57 seconds\n",
      "Epoch 37/1000, Training Loss: 0.0415 Time Left: 38.16 seconds\n",
      "Epoch 38/1000, Training Loss: 0.0425 Time Left: 37.72 seconds\n",
      "Epoch 39/1000, Training Loss: 0.0349 Time Left: 37.25 seconds\n",
      "Epoch 40/1000, Training Loss: 0.0258 Time Left: 36.84 seconds\n",
      "Epoch 41/1000, Training Loss: 0.0340 Time Left: 36.44 seconds\n",
      "Epoch 42/1000, Training Loss: 0.0307 Time Left: 36.09 seconds\n",
      "Epoch 43/1000, Training Loss: 0.0271 Time Left: 35.70 seconds\n",
      "Epoch 44/1000, Training Loss: 0.0275 Time Left: 35.38 seconds\n",
      "Epoch 45/1000, Training Loss: 0.0226 Time Left: 35.05 seconds\n",
      "Epoch 46/1000, Training Loss: 0.0206 Time Left: 34.71 seconds\n",
      "Epoch 47/1000, Training Loss: 0.0400 Time Left: 34.40 seconds\n",
      "Epoch 48/1000, Training Loss: 0.0432 Time Left: 34.09 seconds\n",
      "Epoch 49/1000, Training Loss: 0.0294 Time Left: 33.82 seconds\n",
      "Epoch 50/1000, Training Loss: 0.0306 Time Left: 33.56 seconds\n",
      "Epoch 51/1000, Training Loss: 0.0343 Time Left: 33.35 seconds\n",
      "Epoch 52/1000, Training Loss: 0.0297 Time Left: 33.12 seconds\n",
      "Epoch 53/1000, Training Loss: 0.0288 Time Left: 32.91 seconds\n",
      "Epoch 54/1000, Training Loss: 0.0328 Time Left: 32.68 seconds\n",
      "Epoch 55/1000, Training Loss: 0.0323 Time Left: 32.46 seconds\n",
      "Epoch 56/1000, Training Loss: 0.0283 Time Left: 32.23 seconds\n",
      "Epoch 57/1000, Training Loss: 0.0247 Time Left: 32.05 seconds\n",
      "Epoch 58/1000, Training Loss: 0.0272 Time Left: 31.83 seconds\n",
      "Epoch 59/1000, Training Loss: 0.0278 Time Left: 31.67 seconds\n",
      "Epoch 60/1000, Training Loss: 0.0229 Time Left: 31.65 seconds\n",
      "Epoch 61/1000, Training Loss: 0.0309 Time Left: 31.51 seconds\n",
      "Epoch 62/1000, Training Loss: 0.0319 Time Left: 31.43 seconds\n",
      "Epoch 63/1000, Training Loss: 0.0247 Time Left: 31.31 seconds\n",
      "Epoch 64/1000, Training Loss: 0.0240 Time Left: 31.25 seconds\n",
      "Epoch 65/1000, Training Loss: 0.0278 Time Left: 31.08 seconds\n",
      "Epoch 66/1000, Training Loss: 0.0270 Time Left: 30.99 seconds\n",
      "Epoch 67/1000, Training Loss: 0.0256 Time Left: 30.86 seconds\n",
      "Epoch 68/1000, Training Loss: 0.0271 Time Left: 30.75 seconds\n",
      "Epoch 69/1000, Training Loss: 0.0283 Time Left: 30.65 seconds\n",
      "Epoch 70/1000, Training Loss: 0.0284 Time Left: 30.56 seconds\n",
      "Epoch 71/1000, Training Loss: 0.0243 Time Left: 30.44 seconds\n",
      "Epoch 72/1000, Training Loss: 0.0277 Time Left: 30.35 seconds\n",
      "Epoch 73/1000, Training Loss: 0.0271 Time Left: 30.26 seconds\n",
      "Epoch 74/1000, Training Loss: 0.0266 Time Left: 30.17 seconds\n",
      "Epoch 75/1000, Training Loss: 0.0259 Time Left: 30.08 seconds\n",
      "Epoch 76/1000, Training Loss: 0.0261 Time Left: 30.00 seconds\n",
      "Epoch 77/1000, Training Loss: 0.0318 Time Left: 29.91 seconds\n",
      "Epoch 78/1000, Training Loss: 0.0264 Time Left: 29.84 seconds\n",
      "Epoch 79/1000, Training Loss: 0.0241 Time Left: 29.76 seconds\n",
      "Epoch 80/1000, Training Loss: 0.0257 Time Left: 29.65 seconds\n",
      "Epoch 81/1000, Training Loss: 0.0251 Time Left: 29.56 seconds\n",
      "Epoch 82/1000, Training Loss: 0.0229 Time Left: 29.43 seconds\n",
      "Epoch 83/1000, Training Loss: 0.0235 Time Left: 29.37 seconds\n",
      "Epoch 84/1000, Training Loss: 0.0234 Time Left: 29.28 seconds\n",
      "Epoch 85/1000, Training Loss: 0.0276 Time Left: 29.20 seconds\n",
      "Epoch 86/1000, Training Loss: 0.0245 Time Left: 29.12 seconds\n",
      "Epoch 87/1000, Training Loss: 0.0264 Time Left: 29.03 seconds\n",
      "Epoch 88/1000, Training Loss: 0.0269 Time Left: 29.00 seconds\n",
      "Epoch 89/1000, Training Loss: 0.0245 Time Left: 28.93 seconds\n",
      "Epoch 90/1000, Training Loss: 0.0237 Time Left: 28.85 seconds\n",
      "Epoch 91/1000, Training Loss: 0.0262 Time Left: 28.80 seconds\n",
      "Epoch 92/1000, Training Loss: 0.0314 Time Left: 28.74 seconds\n",
      "Epoch 93/1000, Training Loss: 0.0279 Time Left: 28.67 seconds\n",
      "Epoch 94/1000, Training Loss: 0.0291 Time Left: 28.64 seconds\n",
      "Epoch 95/1000, Training Loss: 0.0269 Time Left: 28.59 seconds\n",
      "Epoch 96/1000, Training Loss: 0.0248 Time Left: 28.52 seconds\n",
      "Epoch 97/1000, Training Loss: 0.0223 Time Left: 28.45 seconds\n",
      "Epoch 98/1000, Training Loss: 0.0224 Time Left: 28.41 seconds\n",
      "Epoch 99/1000, Training Loss: 0.0249 Time Left: 28.38 seconds\n",
      "Epoch 100/1000, Validation Loss: 0.0655\n",
      "New best model found and saved with validation loss: 0.0655\n",
      "Epoch 100/1000, Training Loss: 0.0241 Time Left: 28.97 seconds\n",
      "Epoch 101/1000, Training Loss: 0.0317 Time Left: 28.92 seconds\n",
      "Epoch 102/1000, Training Loss: 0.0277 Time Left: 28.84 seconds\n",
      "Epoch 103/1000, Training Loss: 0.0328 Time Left: 28.79 seconds\n",
      "Epoch 104/1000, Training Loss: 0.0286 Time Left: 28.74 seconds\n",
      "Epoch 105/1000, Training Loss: 0.0239 Time Left: 28.69 seconds\n",
      "Epoch 106/1000, Training Loss: 0.0285 Time Left: 28.63 seconds\n",
      "Epoch 107/1000, Training Loss: 0.0288 Time Left: 28.57 seconds\n",
      "Epoch 108/1000, Training Loss: 0.0254 Time Left: 28.51 seconds\n",
      "Epoch 109/1000, Training Loss: 0.0247 Time Left: 28.45 seconds\n",
      "Epoch 110/1000, Training Loss: 0.0243 Time Left: 28.42 seconds\n",
      "Epoch 111/1000, Training Loss: 0.0303 Time Left: 28.38 seconds\n",
      "Epoch 112/1000, Training Loss: 0.0247 Time Left: 28.33 seconds\n",
      "Epoch 113/1000, Training Loss: 0.0293 Time Left: 28.27 seconds\n",
      "Epoch 114/1000, Training Loss: 0.0245 Time Left: 28.24 seconds\n",
      "Epoch 115/1000, Training Loss: 0.0215 Time Left: 28.18 seconds\n",
      "Epoch 116/1000, Training Loss: 0.0273 Time Left: 28.16 seconds\n",
      "Epoch 117/1000, Training Loss: 0.0238 Time Left: 28.16 seconds\n",
      "Epoch 118/1000, Training Loss: 0.0227 Time Left: 28.13 seconds\n",
      "Epoch 119/1000, Training Loss: 0.0257 Time Left: 28.09 seconds\n",
      "Epoch 120/1000, Training Loss: 0.0202 Time Left: 28.04 seconds\n",
      "Epoch 121/1000, Training Loss: 0.0210 Time Left: 27.98 seconds\n",
      "Epoch 122/1000, Training Loss: 0.0207 Time Left: 27.95 seconds\n",
      "Epoch 123/1000, Training Loss: 0.0220 Time Left: 27.91 seconds\n",
      "Epoch 124/1000, Training Loss: 0.0238 Time Left: 27.86 seconds\n",
      "Epoch 125/1000, Training Loss: 0.0265 Time Left: 27.82 seconds\n",
      "Epoch 126/1000, Training Loss: 0.0267 Time Left: 27.79 seconds\n",
      "Epoch 127/1000, Training Loss: 0.0264 Time Left: 27.74 seconds\n",
      "Epoch 128/1000, Training Loss: 0.0216 Time Left: 27.69 seconds\n",
      "Epoch 129/1000, Training Loss: 0.0323 Time Left: 27.64 seconds\n",
      "Epoch 130/1000, Training Loss: 0.0214 Time Left: 27.61 seconds\n",
      "Epoch 131/1000, Training Loss: 0.0231 Time Left: 27.60 seconds\n",
      "Epoch 132/1000, Training Loss: 0.0190 Time Left: 27.56 seconds\n",
      "Epoch 133/1000, Training Loss: 0.0261 Time Left: 27.53 seconds\n",
      "Epoch 134/1000, Training Loss: 0.0315 Time Left: 27.48 seconds\n",
      "Epoch 135/1000, Training Loss: 0.0271 Time Left: 27.44 seconds\n",
      "Epoch 136/1000, Training Loss: 0.0274 Time Left: 27.41 seconds\n",
      "Epoch 137/1000, Training Loss: 0.0254 Time Left: 27.36 seconds\n",
      "Epoch 138/1000, Training Loss: 0.0285 Time Left: 27.35 seconds\n",
      "Epoch 139/1000, Training Loss: 0.0169 Time Left: 27.32 seconds\n",
      "Epoch 140/1000, Training Loss: 0.0341 Time Left: 27.31 seconds\n",
      "Epoch 141/1000, Training Loss: 0.0457 Time Left: 27.33 seconds\n",
      "Epoch 142/1000, Training Loss: 0.0288 Time Left: 27.33 seconds\n",
      "Epoch 143/1000, Training Loss: 0.0338 Time Left: 27.28 seconds\n",
      "Epoch 144/1000, Training Loss: 0.0334 Time Left: 27.23 seconds\n",
      "Epoch 145/1000, Training Loss: 0.0300 Time Left: 27.19 seconds\n",
      "Epoch 146/1000, Training Loss: 0.0255 Time Left: 27.14 seconds\n",
      "Epoch 147/1000, Training Loss: 0.0246 Time Left: 27.11 seconds\n",
      "Epoch 148/1000, Training Loss: 0.0414 Time Left: 27.06 seconds\n",
      "Epoch 149/1000, Training Loss: 0.0288 Time Left: 27.02 seconds\n",
      "Epoch 150/1000, Training Loss: 0.0257 Time Left: 26.98 seconds\n",
      "Epoch 151/1000, Training Loss: 0.0301 Time Left: 26.94 seconds\n",
      "Epoch 152/1000, Training Loss: 0.0318 Time Left: 26.90 seconds\n",
      "Epoch 153/1000, Training Loss: 0.0243 Time Left: 26.86 seconds\n",
      "Epoch 154/1000, Training Loss: 0.0252 Time Left: 26.82 seconds\n",
      "Epoch 155/1000, Training Loss: 0.0216 Time Left: 26.79 seconds\n",
      "Epoch 156/1000, Training Loss: 0.0263 Time Left: 26.75 seconds\n",
      "Epoch 157/1000, Training Loss: 0.0230 Time Left: 26.71 seconds\n",
      "Epoch 158/1000, Training Loss: 0.0291 Time Left: 26.67 seconds\n",
      "Epoch 159/1000, Training Loss: 0.0250 Time Left: 26.66 seconds\n",
      "Epoch 160/1000, Training Loss: 0.0236 Time Left: 26.62 seconds\n",
      "Epoch 161/1000, Training Loss: 0.0220 Time Left: 26.58 seconds\n",
      "Epoch 162/1000, Training Loss: 0.0297 Time Left: 26.54 seconds\n",
      "Epoch 163/1000, Training Loss: 0.0297 Time Left: 26.50 seconds\n",
      "Epoch 164/1000, Training Loss: 0.0262 Time Left: 26.46 seconds\n",
      "Epoch 165/1000, Training Loss: 0.0214 Time Left: 26.42 seconds\n",
      "Epoch 166/1000, Training Loss: 0.0210 Time Left: 26.38 seconds\n",
      "Epoch 167/1000, Training Loss: 0.0332 Time Left: 26.34 seconds\n",
      "Epoch 168/1000, Training Loss: 0.0298 Time Left: 26.31 seconds\n",
      "Epoch 169/1000, Training Loss: 0.0299 Time Left: 26.27 seconds\n",
      "Epoch 170/1000, Training Loss: 0.0283 Time Left: 26.23 seconds\n",
      "Epoch 171/1000, Training Loss: 0.0293 Time Left: 26.19 seconds\n",
      "Epoch 172/1000, Training Loss: 0.0250 Time Left: 26.14 seconds\n",
      "Epoch 173/1000, Training Loss: 0.0252 Time Left: 26.10 seconds\n",
      "Epoch 174/1000, Training Loss: 0.0208 Time Left: 26.05 seconds\n",
      "Epoch 175/1000, Training Loss: 0.0295 Time Left: 26.04 seconds\n",
      "Epoch 176/1000, Training Loss: 0.0248 Time Left: 26.00 seconds\n",
      "Epoch 177/1000, Training Loss: 0.0206 Time Left: 26.00 seconds\n",
      "Epoch 178/1000, Training Loss: 0.0181 Time Left: 25.98 seconds\n",
      "Epoch 179/1000, Training Loss: 0.0221 Time Left: 25.94 seconds\n",
      "Epoch 180/1000, Training Loss: 0.0232 Time Left: 25.90 seconds\n",
      "Epoch 181/1000, Training Loss: 0.0278 Time Left: 25.87 seconds\n",
      "Epoch 182/1000, Training Loss: 0.0242 Time Left: 25.83 seconds\n",
      "Epoch 183/1000, Training Loss: 0.0242 Time Left: 25.79 seconds\n",
      "Epoch 184/1000, Training Loss: 0.0232 Time Left: 25.75 seconds\n",
      "Epoch 185/1000, Training Loss: 0.0252 Time Left: 25.71 seconds\n",
      "Epoch 186/1000, Training Loss: 0.0251 Time Left: 25.67 seconds\n",
      "Epoch 187/1000, Training Loss: 0.0260 Time Left: 25.63 seconds\n",
      "Epoch 188/1000, Training Loss: 0.0261 Time Left: 25.58 seconds\n",
      "Epoch 189/1000, Training Loss: 0.0279 Time Left: 25.56 seconds\n",
      "Epoch 190/1000, Training Loss: 0.0247 Time Left: 25.53 seconds\n",
      "Epoch 191/1000, Training Loss: 0.0253 Time Left: 25.49 seconds\n",
      "Epoch 192/1000, Training Loss: 0.0245 Time Left: 25.45 seconds\n",
      "Epoch 193/1000, Training Loss: 0.0233 Time Left: 25.43 seconds\n",
      "Epoch 194/1000, Training Loss: 0.0238 Time Left: 25.41 seconds\n",
      "Epoch 195/1000, Training Loss: 0.0238 Time Left: 25.38 seconds\n",
      "Epoch 196/1000, Training Loss: 0.0177 Time Left: 25.37 seconds\n",
      "Epoch 197/1000, Training Loss: 0.0265 Time Left: 25.33 seconds\n",
      "Epoch 198/1000, Training Loss: 0.0197 Time Left: 25.30 seconds\n",
      "Epoch 199/1000, Training Loss: 0.0185 Time Left: 25.26 seconds\n",
      "Epoch 200/1000, Validation Loss: 0.0870\n",
      "Epoch 200/1000, Training Loss: 0.0196 Time Left: 25.28 seconds\n",
      "Epoch 201/1000, Training Loss: 0.0200 Time Left: 25.25 seconds\n",
      "Epoch 202/1000, Training Loss: 0.0210 Time Left: 25.23 seconds\n",
      "Epoch 203/1000, Training Loss: 0.0257 Time Left: 25.20 seconds\n",
      "Epoch 204/1000, Training Loss: 0.0258 Time Left: 25.17 seconds\n",
      "Epoch 205/1000, Training Loss: 0.0259 Time Left: 25.13 seconds\n",
      "Epoch 206/1000, Training Loss: 0.0219 Time Left: 25.09 seconds\n",
      "Epoch 207/1000, Training Loss: 0.0197 Time Left: 25.07 seconds\n",
      "Epoch 208/1000, Training Loss: 0.0228 Time Left: 25.05 seconds\n",
      "Epoch 209/1000, Training Loss: 0.0315 Time Left: 25.03 seconds\n",
      "Epoch 210/1000, Training Loss: 0.0172 Time Left: 25.00 seconds\n",
      "Epoch 211/1000, Training Loss: 0.0227 Time Left: 24.96 seconds\n",
      "Epoch 212/1000, Training Loss: 0.0210 Time Left: 24.94 seconds\n",
      "Epoch 213/1000, Training Loss: 0.0164 Time Left: 24.91 seconds\n",
      "Epoch 214/1000, Training Loss: 0.0278 Time Left: 24.90 seconds\n",
      "Epoch 215/1000, Training Loss: 0.0211 Time Left: 24.87 seconds\n",
      "Epoch 216/1000, Training Loss: 0.0237 Time Left: 24.85 seconds\n",
      "Epoch 217/1000, Training Loss: 0.0265 Time Left: 24.82 seconds\n",
      "Epoch 218/1000, Training Loss: 0.0215 Time Left: 24.80 seconds\n",
      "Epoch 219/1000, Training Loss: 0.0225 Time Left: 24.77 seconds\n",
      "Epoch 220/1000, Training Loss: 0.0325 Time Left: 24.75 seconds\n",
      "Epoch 221/1000, Training Loss: 0.0260 Time Left: 24.71 seconds\n",
      "Epoch 222/1000, Training Loss: 0.0177 Time Left: 24.70 seconds\n",
      "Epoch 223/1000, Training Loss: 0.0233 Time Left: 24.67 seconds\n",
      "Epoch 224/1000, Training Loss: 0.0250 Time Left: 24.64 seconds\n",
      "Epoch 225/1000, Training Loss: 0.0219 Time Left: 24.62 seconds\n",
      "Epoch 226/1000, Training Loss: 0.0236 Time Left: 24.59 seconds\n",
      "Epoch 227/1000, Training Loss: 0.0175 Time Left: 24.55 seconds\n",
      "Epoch 228/1000, Training Loss: 0.0219 Time Left: 24.53 seconds\n",
      "Epoch 229/1000, Training Loss: 0.0272 Time Left: 24.50 seconds\n",
      "Epoch 230/1000, Training Loss: 0.0242 Time Left: 24.48 seconds\n",
      "Epoch 231/1000, Training Loss: 0.0199 Time Left: 24.45 seconds\n",
      "Epoch 232/1000, Training Loss: 0.0313 Time Left: 24.43 seconds\n",
      "Epoch 233/1000, Training Loss: 0.0287 Time Left: 24.39 seconds\n",
      "Epoch 234/1000, Training Loss: 0.0261 Time Left: 24.36 seconds\n",
      "Epoch 235/1000, Training Loss: 0.0219 Time Left: 24.34 seconds\n",
      "Epoch 236/1000, Training Loss: 0.0226 Time Left: 24.31 seconds\n",
      "Epoch 237/1000, Training Loss: 0.0241 Time Left: 24.29 seconds\n",
      "Epoch 238/1000, Training Loss: 0.0269 Time Left: 24.25 seconds\n",
      "Epoch 239/1000, Training Loss: 0.0194 Time Left: 24.22 seconds\n",
      "Epoch 240/1000, Training Loss: 0.0217 Time Left: 24.19 seconds\n",
      "Epoch 241/1000, Training Loss: 0.0195 Time Left: 24.16 seconds\n",
      "Epoch 242/1000, Training Loss: 0.0335 Time Left: 24.14 seconds\n",
      "Epoch 243/1000, Training Loss: 0.0255 Time Left: 24.10 seconds\n",
      "Epoch 244/1000, Training Loss: 0.0240 Time Left: 24.08 seconds\n",
      "Epoch 245/1000, Training Loss: 0.0295 Time Left: 24.05 seconds\n",
      "Epoch 246/1000, Training Loss: 0.0238 Time Left: 24.02 seconds\n",
      "Epoch 247/1000, Training Loss: 0.0203 Time Left: 23.99 seconds\n",
      "Epoch 248/1000, Training Loss: 0.0160 Time Left: 23.95 seconds\n",
      "Epoch 249/1000, Training Loss: 0.0221 Time Left: 23.91 seconds\n",
      "Epoch 250/1000, Training Loss: 0.0270 Time Left: 23.89 seconds\n",
      "Epoch 251/1000, Training Loss: 0.0202 Time Left: 23.86 seconds\n",
      "Epoch 252/1000, Training Loss: 0.0212 Time Left: 23.83 seconds\n",
      "Epoch 253/1000, Training Loss: 0.0257 Time Left: 23.80 seconds\n",
      "Epoch 254/1000, Training Loss: 0.0257 Time Left: 23.77 seconds\n",
      "Epoch 255/1000, Training Loss: 0.0282 Time Left: 23.75 seconds\n",
      "Epoch 256/1000, Training Loss: 0.0247 Time Left: 23.72 seconds\n",
      "Epoch 257/1000, Training Loss: 0.0190 Time Left: 23.70 seconds\n",
      "Epoch 258/1000, Training Loss: 0.0234 Time Left: 23.67 seconds\n",
      "Epoch 259/1000, Training Loss: 0.0215 Time Left: 23.64 seconds\n",
      "Epoch 260/1000, Training Loss: 0.0265 Time Left: 23.60 seconds\n",
      "Epoch 261/1000, Training Loss: 0.0251 Time Left: 23.58 seconds\n",
      "Epoch 262/1000, Training Loss: 0.0147 Time Left: 23.55 seconds\n",
      "Epoch 263/1000, Training Loss: 0.0262 Time Left: 23.51 seconds\n",
      "Epoch 264/1000, Training Loss: 0.0195 Time Left: 23.48 seconds\n",
      "Epoch 265/1000, Training Loss: 0.0160 Time Left: 23.45 seconds\n",
      "Epoch 266/1000, Training Loss: 0.0160 Time Left: 23.42 seconds\n",
      "Epoch 267/1000, Training Loss: 0.0367 Time Left: 23.39 seconds\n",
      "Epoch 268/1000, Training Loss: 0.0286 Time Left: 23.36 seconds\n",
      "Epoch 269/1000, Training Loss: 0.0340 Time Left: 23.33 seconds\n",
      "Epoch 270/1000, Training Loss: 0.0223 Time Left: 23.30 seconds\n",
      "Epoch 271/1000, Training Loss: 0.0269 Time Left: 23.27 seconds\n",
      "Epoch 272/1000, Training Loss: 0.0205 Time Left: 23.24 seconds\n",
      "Epoch 273/1000, Training Loss: 0.0235 Time Left: 23.20 seconds\n",
      "Epoch 274/1000, Training Loss: 0.0216 Time Left: 23.17 seconds\n",
      "Epoch 275/1000, Training Loss: 0.0290 Time Left: 23.13 seconds\n",
      "Epoch 276/1000, Training Loss: 0.0337 Time Left: 23.10 seconds\n",
      "Epoch 277/1000, Training Loss: 0.0247 Time Left: 23.07 seconds\n",
      "Epoch 278/1000, Training Loss: 0.0218 Time Left: 23.04 seconds\n",
      "Epoch 279/1000, Training Loss: 0.0112 Time Left: 23.02 seconds\n",
      "Epoch 280/1000, Training Loss: 0.0431 Time Left: 22.98 seconds\n",
      "Epoch 281/1000, Training Loss: 0.0222 Time Left: 22.95 seconds\n",
      "Epoch 282/1000, Training Loss: 0.0250 Time Left: 22.93 seconds\n",
      "Epoch 283/1000, Training Loss: 0.0255 Time Left: 22.89 seconds\n",
      "Epoch 284/1000, Training Loss: 0.0220 Time Left: 22.86 seconds\n",
      "Epoch 285/1000, Training Loss: 0.0320 Time Left: 22.83 seconds\n",
      "Epoch 286/1000, Training Loss: 0.0216 Time Left: 22.79 seconds\n",
      "Epoch 287/1000, Training Loss: 0.0271 Time Left: 22.76 seconds\n",
      "Epoch 288/1000, Training Loss: 0.0278 Time Left: 22.73 seconds\n",
      "Epoch 289/1000, Training Loss: 0.0255 Time Left: 22.69 seconds\n",
      "Epoch 290/1000, Training Loss: 0.0239 Time Left: 22.65 seconds\n",
      "Epoch 291/1000, Training Loss: 0.0396 Time Left: 22.62 seconds\n",
      "Epoch 292/1000, Training Loss: 0.0219 Time Left: 22.58 seconds\n",
      "Epoch 293/1000, Training Loss: 0.0265 Time Left: 22.55 seconds\n",
      "Epoch 294/1000, Training Loss: 0.0192 Time Left: 22.51 seconds\n",
      "Epoch 295/1000, Training Loss: 0.0166 Time Left: 22.48 seconds\n",
      "Epoch 296/1000, Training Loss: 0.0211 Time Left: 22.44 seconds\n",
      "Epoch 297/1000, Training Loss: 0.0317 Time Left: 22.41 seconds\n",
      "Epoch 298/1000, Training Loss: 0.0213 Time Left: 22.36 seconds\n",
      "Epoch 299/1000, Training Loss: 0.0285 Time Left: 22.33 seconds\n",
      "Epoch 300/1000, Validation Loss: 0.0710\n",
      "Epoch 300/1000, Training Loss: 0.0308 Time Left: 22.31 seconds\n",
      "Epoch 301/1000, Training Loss: 0.0246 Time Left: 22.27 seconds\n",
      "Epoch 302/1000, Training Loss: 0.0202 Time Left: 22.23 seconds\n",
      "Epoch 303/1000, Training Loss: 0.0224 Time Left: 22.21 seconds\n",
      "Epoch 304/1000, Training Loss: 0.0533 Time Left: 22.17 seconds\n",
      "Epoch 305/1000, Training Loss: 0.0238 Time Left: 22.13 seconds\n",
      "Epoch 306/1000, Training Loss: 0.0264 Time Left: 22.09 seconds\n",
      "Epoch 307/1000, Training Loss: 0.0280 Time Left: 22.06 seconds\n",
      "Epoch 308/1000, Training Loss: 0.0319 Time Left: 22.03 seconds\n",
      "Epoch 309/1000, Training Loss: 0.0289 Time Left: 21.99 seconds\n",
      "Epoch 310/1000, Training Loss: 0.0245 Time Left: 21.96 seconds\n",
      "Epoch 311/1000, Training Loss: 0.0296 Time Left: 21.92 seconds\n",
      "Epoch 312/1000, Training Loss: 0.0200 Time Left: 21.89 seconds\n",
      "Epoch 313/1000, Training Loss: 0.0289 Time Left: 21.86 seconds\n",
      "Epoch 314/1000, Training Loss: 0.0204 Time Left: 21.83 seconds\n",
      "Epoch 315/1000, Training Loss: 0.0196 Time Left: 21.79 seconds\n",
      "Epoch 316/1000, Training Loss: 0.0308 Time Left: 21.75 seconds\n",
      "Epoch 317/1000, Training Loss: 0.0171 Time Left: 21.72 seconds\n",
      "Epoch 318/1000, Training Loss: 0.0304 Time Left: 21.70 seconds\n",
      "Epoch 319/1000, Training Loss: 0.0217 Time Left: 21.67 seconds\n",
      "Epoch 320/1000, Training Loss: 0.0294 Time Left: 21.64 seconds\n",
      "Epoch 321/1000, Training Loss: 0.0246 Time Left: 21.60 seconds\n",
      "Epoch 322/1000, Training Loss: 0.0201 Time Left: 21.57 seconds\n",
      "Epoch 323/1000, Training Loss: 0.0231 Time Left: 21.54 seconds\n",
      "Epoch 324/1000, Training Loss: 0.0274 Time Left: 21.50 seconds\n",
      "Epoch 325/1000, Training Loss: 0.0276 Time Left: 21.47 seconds\n",
      "Epoch 326/1000, Training Loss: 0.0247 Time Left: 21.45 seconds\n",
      "Epoch 327/1000, Training Loss: 0.0269 Time Left: 21.42 seconds\n",
      "Epoch 328/1000, Training Loss: 0.0279 Time Left: 21.38 seconds\n",
      "Epoch 329/1000, Training Loss: 0.0190 Time Left: 21.35 seconds\n",
      "Epoch 330/1000, Training Loss: 0.0223 Time Left: 21.32 seconds\n",
      "Epoch 331/1000, Training Loss: 0.0215 Time Left: 21.29 seconds\n",
      "Epoch 332/1000, Training Loss: 0.0180 Time Left: 21.25 seconds\n",
      "Epoch 333/1000, Training Loss: 0.0267 Time Left: 21.22 seconds\n",
      "Epoch 334/1000, Training Loss: 0.0202 Time Left: 21.19 seconds\n",
      "Epoch 335/1000, Training Loss: 0.0223 Time Left: 21.16 seconds\n",
      "Epoch 336/1000, Training Loss: 0.0191 Time Left: 21.12 seconds\n",
      "Epoch 337/1000, Training Loss: 0.0245 Time Left: 21.09 seconds\n",
      "Epoch 338/1000, Training Loss: 0.0637 Time Left: 21.06 seconds\n",
      "Epoch 339/1000, Training Loss: 0.0202 Time Left: 21.03 seconds\n",
      "Epoch 340/1000, Training Loss: 0.0315 Time Left: 20.99 seconds\n",
      "Epoch 341/1000, Training Loss: 0.0289 Time Left: 20.97 seconds\n",
      "Epoch 342/1000, Training Loss: 0.0246 Time Left: 20.93 seconds\n",
      "Epoch 343/1000, Training Loss: 0.0207 Time Left: 20.91 seconds\n",
      "Epoch 344/1000, Training Loss: 0.0270 Time Left: 20.87 seconds\n",
      "Epoch 345/1000, Training Loss: 0.0181 Time Left: 20.84 seconds\n",
      "Epoch 346/1000, Training Loss: 0.0269 Time Left: 20.81 seconds\n",
      "Epoch 347/1000, Training Loss: 0.0278 Time Left: 20.78 seconds\n",
      "Epoch 348/1000, Training Loss: 0.0193 Time Left: 20.76 seconds\n",
      "Epoch 349/1000, Training Loss: 0.0204 Time Left: 20.73 seconds\n",
      "Epoch 350/1000, Training Loss: 0.0230 Time Left: 20.71 seconds\n",
      "Epoch 351/1000, Training Loss: 0.0298 Time Left: 20.67 seconds\n",
      "Epoch 352/1000, Training Loss: 0.0259 Time Left: 20.65 seconds\n",
      "Epoch 353/1000, Training Loss: 0.0533 Time Left: 20.63 seconds\n",
      "Epoch 354/1000, Training Loss: 0.0270 Time Left: 20.61 seconds\n",
      "Epoch 355/1000, Training Loss: 0.0282 Time Left: 20.59 seconds\n",
      "Epoch 356/1000, Training Loss: 0.0248 Time Left: 20.56 seconds\n",
      "Epoch 357/1000, Training Loss: 0.0218 Time Left: 20.53 seconds\n",
      "Epoch 358/1000, Training Loss: 0.0245 Time Left: 20.50 seconds\n",
      "Epoch 359/1000, Training Loss: 0.0257 Time Left: 20.48 seconds\n",
      "Epoch 360/1000, Training Loss: 0.0284 Time Left: 20.45 seconds\n",
      "Epoch 361/1000, Training Loss: 0.0277 Time Left: 20.43 seconds\n",
      "Epoch 362/1000, Training Loss: 0.0259 Time Left: 20.40 seconds\n",
      "Epoch 363/1000, Training Loss: 0.0340 Time Left: 20.37 seconds\n",
      "Epoch 364/1000, Training Loss: 0.0239 Time Left: 20.34 seconds\n",
      "Epoch 365/1000, Training Loss: 0.0239 Time Left: 20.31 seconds\n",
      "Epoch 366/1000, Training Loss: 0.0224 Time Left: 20.28 seconds\n",
      "Epoch 367/1000, Training Loss: 0.0216 Time Left: 20.26 seconds\n",
      "Epoch 368/1000, Training Loss: 0.0231 Time Left: 20.23 seconds\n",
      "Epoch 369/1000, Training Loss: 0.0254 Time Left: 20.20 seconds\n",
      "Epoch 370/1000, Training Loss: 0.0250 Time Left: 20.18 seconds\n",
      "Epoch 371/1000, Training Loss: 0.0197 Time Left: 20.14 seconds\n",
      "Epoch 372/1000, Training Loss: 0.0306 Time Left: 20.11 seconds\n",
      "Epoch 373/1000, Training Loss: 0.0261 Time Left: 20.08 seconds\n",
      "Epoch 374/1000, Training Loss: 0.0222 Time Left: 20.05 seconds\n",
      "Epoch 375/1000, Training Loss: 0.0253 Time Left: 20.02 seconds\n",
      "Epoch 376/1000, Training Loss: 0.0250 Time Left: 20.00 seconds\n",
      "Epoch 377/1000, Training Loss: 0.0229 Time Left: 19.98 seconds\n",
      "Epoch 378/1000, Training Loss: 0.0195 Time Left: 19.95 seconds\n",
      "Epoch 379/1000, Training Loss: 0.0196 Time Left: 19.92 seconds\n",
      "Epoch 380/1000, Training Loss: 0.0215 Time Left: 19.89 seconds\n",
      "Epoch 381/1000, Training Loss: 0.0212 Time Left: 19.87 seconds\n",
      "Epoch 382/1000, Training Loss: 0.0301 Time Left: 19.84 seconds\n",
      "Epoch 383/1000, Training Loss: 0.0285 Time Left: 19.81 seconds\n",
      "Epoch 384/1000, Training Loss: 0.0193 Time Left: 19.78 seconds\n",
      "Epoch 385/1000, Training Loss: 0.0214 Time Left: 19.75 seconds\n",
      "Epoch 386/1000, Training Loss: 0.0276 Time Left: 19.72 seconds\n",
      "Epoch 387/1000, Training Loss: 0.0180 Time Left: 19.69 seconds\n",
      "Epoch 388/1000, Training Loss: 0.0195 Time Left: 19.66 seconds\n",
      "Epoch 389/1000, Training Loss: 0.0151 Time Left: 19.63 seconds\n",
      "Epoch 390/1000, Training Loss: 0.0233 Time Left: 19.60 seconds\n",
      "Epoch 391/1000, Training Loss: 0.0219 Time Left: 19.58 seconds\n",
      "Epoch 392/1000, Training Loss: 0.0200 Time Left: 19.55 seconds\n",
      "Epoch 393/1000, Training Loss: 0.0191 Time Left: 19.52 seconds\n",
      "Epoch 394/1000, Training Loss: 0.0224 Time Left: 19.50 seconds\n",
      "Epoch 395/1000, Training Loss: 0.0322 Time Left: 19.46 seconds\n",
      "Epoch 396/1000, Training Loss: 0.0212 Time Left: 19.43 seconds\n",
      "Epoch 397/1000, Training Loss: 0.0292 Time Left: 19.40 seconds\n",
      "Epoch 398/1000, Training Loss: 0.0229 Time Left: 19.37 seconds\n",
      "Epoch 399/1000, Training Loss: 0.0187 Time Left: 19.34 seconds\n",
      "Epoch 400/1000, Validation Loss: 0.0792\n",
      "Epoch 400/1000, Training Loss: 0.0192 Time Left: 19.32 seconds\n",
      "Epoch 401/1000, Training Loss: 0.0247 Time Left: 19.29 seconds\n",
      "Epoch 402/1000, Training Loss: 0.0293 Time Left: 19.26 seconds\n",
      "Epoch 403/1000, Training Loss: 0.0218 Time Left: 19.24 seconds\n",
      "Epoch 404/1000, Training Loss: 0.0241 Time Left: 19.21 seconds\n",
      "Epoch 405/1000, Training Loss: 0.0236 Time Left: 19.19 seconds\n",
      "Epoch 406/1000, Training Loss: 0.0217 Time Left: 19.17 seconds\n",
      "Epoch 407/1000, Training Loss: 0.0214 Time Left: 19.15 seconds\n",
      "Epoch 408/1000, Training Loss: 0.0210 Time Left: 19.13 seconds\n",
      "Epoch 409/1000, Training Loss: 0.0195 Time Left: 19.10 seconds\n",
      "Epoch 410/1000, Training Loss: 0.0218 Time Left: 19.08 seconds\n",
      "Epoch 411/1000, Training Loss: 0.0199 Time Left: 19.06 seconds\n",
      "Epoch 412/1000, Training Loss: 0.0175 Time Left: 19.04 seconds\n",
      "Epoch 413/1000, Training Loss: 0.0250 Time Left: 19.01 seconds\n",
      "Epoch 414/1000, Training Loss: 0.0236 Time Left: 18.99 seconds\n",
      "Epoch 415/1000, Training Loss: 0.0117 Time Left: 18.95 seconds\n",
      "Epoch 416/1000, Training Loss: 0.0217 Time Left: 18.94 seconds\n",
      "Epoch 417/1000, Training Loss: 0.0355 Time Left: 18.92 seconds\n",
      "Epoch 418/1000, Training Loss: 0.0219 Time Left: 18.89 seconds\n",
      "Epoch 419/1000, Training Loss: 0.0252 Time Left: 18.87 seconds\n",
      "Epoch 420/1000, Training Loss: 0.0230 Time Left: 18.84 seconds\n",
      "Epoch 421/1000, Training Loss: 0.0234 Time Left: 18.81 seconds\n",
      "Epoch 422/1000, Training Loss: 0.0214 Time Left: 18.78 seconds\n",
      "Epoch 423/1000, Training Loss: 0.0191 Time Left: 18.75 seconds\n",
      "Epoch 424/1000, Training Loss: 0.0214 Time Left: 18.72 seconds\n",
      "Epoch 425/1000, Training Loss: 0.0226 Time Left: 18.69 seconds\n",
      "Epoch 426/1000, Training Loss: 0.0174 Time Left: 18.66 seconds\n",
      "Epoch 427/1000, Training Loss: 0.0245 Time Left: 18.63 seconds\n",
      "Epoch 428/1000, Training Loss: 0.0227 Time Left: 18.60 seconds\n",
      "Epoch 429/1000, Training Loss: 0.0267 Time Left: 18.57 seconds\n",
      "Epoch 430/1000, Training Loss: 0.0211 Time Left: 18.54 seconds\n",
      "Epoch 431/1000, Training Loss: 0.0212 Time Left: 18.51 seconds\n",
      "Epoch 432/1000, Training Loss: 0.0300 Time Left: 18.47 seconds\n",
      "Epoch 433/1000, Training Loss: 0.0203 Time Left: 18.44 seconds\n",
      "Epoch 434/1000, Training Loss: 0.0259 Time Left: 18.42 seconds\n",
      "Epoch 435/1000, Training Loss: 0.0214 Time Left: 18.39 seconds\n",
      "Epoch 436/1000, Training Loss: 0.0223 Time Left: 18.37 seconds\n",
      "Epoch 437/1000, Training Loss: 0.1507 Time Left: 18.34 seconds\n",
      "Epoch 438/1000, Training Loss: 0.0297 Time Left: 18.32 seconds\n",
      "Epoch 439/1000, Training Loss: 0.0353 Time Left: 18.29 seconds\n",
      "Epoch 440/1000, Training Loss: 0.0268 Time Left: 18.26 seconds\n",
      "Epoch 441/1000, Training Loss: 0.0368 Time Left: 18.24 seconds\n",
      "Epoch 442/1000, Training Loss: 0.0267 Time Left: 18.21 seconds\n",
      "Epoch 443/1000, Training Loss: 0.0249 Time Left: 18.17 seconds\n",
      "Epoch 444/1000, Training Loss: 0.0192 Time Left: 18.14 seconds\n",
      "Epoch 445/1000, Training Loss: 0.0183 Time Left: 18.11 seconds\n",
      "Epoch 446/1000, Training Loss: 0.0219 Time Left: 18.07 seconds\n",
      "Epoch 447/1000, Training Loss: 0.0243 Time Left: 18.04 seconds\n",
      "Epoch 448/1000, Training Loss: 0.0259 Time Left: 18.00 seconds\n",
      "Epoch 449/1000, Training Loss: 0.0251 Time Left: 17.97 seconds\n",
      "Epoch 450/1000, Training Loss: 0.0241 Time Left: 17.94 seconds\n",
      "Epoch 451/1000, Training Loss: 0.0226 Time Left: 17.91 seconds\n",
      "Epoch 452/1000, Training Loss: 0.0261 Time Left: 17.87 seconds\n",
      "Epoch 453/1000, Training Loss: 0.0255 Time Left: 17.84 seconds\n",
      "Epoch 454/1000, Training Loss: 0.0296 Time Left: 17.81 seconds\n",
      "Epoch 455/1000, Training Loss: 0.0246 Time Left: 17.78 seconds\n",
      "Epoch 456/1000, Training Loss: 0.0228 Time Left: 17.75 seconds\n",
      "Epoch 457/1000, Training Loss: 0.0157 Time Left: 17.72 seconds\n",
      "Epoch 458/1000, Training Loss: 0.0252 Time Left: 17.68 seconds\n",
      "Epoch 459/1000, Training Loss: 0.0228 Time Left: 17.65 seconds\n",
      "Epoch 460/1000, Training Loss: 0.0273 Time Left: 17.62 seconds\n",
      "Epoch 461/1000, Training Loss: 0.0212 Time Left: 17.59 seconds\n",
      "Epoch 462/1000, Training Loss: 0.0236 Time Left: 17.56 seconds\n",
      "Epoch 463/1000, Training Loss: 0.0278 Time Left: 17.53 seconds\n",
      "Epoch 464/1000, Training Loss: 0.0233 Time Left: 17.49 seconds\n",
      "Epoch 465/1000, Training Loss: 0.0206 Time Left: 17.46 seconds\n",
      "Epoch 466/1000, Training Loss: 0.0200 Time Left: 17.43 seconds\n",
      "Epoch 467/1000, Training Loss: 0.0229 Time Left: 17.40 seconds\n",
      "Epoch 468/1000, Training Loss: 0.0214 Time Left: 17.36 seconds\n",
      "Epoch 469/1000, Training Loss: 0.0239 Time Left: 17.33 seconds\n",
      "Epoch 470/1000, Training Loss: 0.0226 Time Left: 17.30 seconds\n",
      "Epoch 471/1000, Training Loss: 0.0201 Time Left: 17.27 seconds\n",
      "Epoch 472/1000, Training Loss: 0.0278 Time Left: 17.24 seconds\n",
      "Epoch 473/1000, Training Loss: 2487.7587 Time Left: 17.21 seconds\n",
      "Epoch 474/1000, Training Loss: 0.0290 Time Left: 17.18 seconds\n",
      "Epoch 475/1000, Training Loss: 0.0266 Time Left: 17.15 seconds\n",
      "Epoch 476/1000, Training Loss: 0.0268 Time Left: 17.11 seconds\n",
      "Epoch 477/1000, Training Loss: 0.0252 Time Left: 17.08 seconds\n",
      "Epoch 478/1000, Training Loss: 0.0307 Time Left: 17.05 seconds\n",
      "Epoch 479/1000, Training Loss: 0.0166 Time Left: 17.01 seconds\n",
      "Epoch 480/1000, Training Loss: 0.0372 Time Left: 16.98 seconds\n",
      "Epoch 481/1000, Training Loss: 0.0332 Time Left: 16.95 seconds\n",
      "Epoch 482/1000, Training Loss: 0.0259 Time Left: 16.92 seconds\n",
      "Epoch 483/1000, Training Loss: 0.0271 Time Left: 16.88 seconds\n",
      "Epoch 484/1000, Training Loss: 0.0204 Time Left: 16.85 seconds\n",
      "Epoch 485/1000, Training Loss: 0.0202 Time Left: 16.82 seconds\n",
      "Epoch 486/1000, Training Loss: 0.0196 Time Left: 16.78 seconds\n",
      "Epoch 487/1000, Training Loss: 0.0205 Time Left: 16.75 seconds\n",
      "Epoch 488/1000, Training Loss: 0.0241 Time Left: 16.72 seconds\n",
      "Epoch 489/1000, Training Loss: 0.0462 Time Left: 16.68 seconds\n",
      "Epoch 490/1000, Training Loss: 0.0335 Time Left: 16.65 seconds\n",
      "Epoch 491/1000, Training Loss: 0.0288 Time Left: 16.62 seconds\n",
      "Epoch 492/1000, Training Loss: 0.0236 Time Left: 16.59 seconds\n",
      "Epoch 493/1000, Training Loss: 0.0193 Time Left: 16.56 seconds\n",
      "Epoch 494/1000, Training Loss: 0.0331 Time Left: 16.53 seconds\n",
      "Epoch 495/1000, Training Loss: 0.0382 Time Left: 16.50 seconds\n",
      "Epoch 496/1000, Training Loss: 0.0353 Time Left: 16.47 seconds\n",
      "Epoch 497/1000, Training Loss: 0.0260 Time Left: 16.43 seconds\n",
      "Epoch 498/1000, Training Loss: 0.0185 Time Left: 16.40 seconds\n",
      "Epoch 499/1000, Training Loss: 0.0180 Time Left: 16.37 seconds\n",
      "Epoch 500/1000, Validation Loss: 0.0821\n",
      "Epoch 500/1000, Training Loss: 0.0289 Time Left: 16.35 seconds\n",
      "Epoch 501/1000, Training Loss: 0.0223 Time Left: 16.31 seconds\n",
      "Epoch 502/1000, Training Loss: 0.0253 Time Left: 16.28 seconds\n",
      "Epoch 503/1000, Training Loss: 0.0268 Time Left: 16.25 seconds\n",
      "Epoch 504/1000, Training Loss: 0.0206 Time Left: 16.22 seconds\n",
      "Epoch 505/1000, Training Loss: 0.0298 Time Left: 16.19 seconds\n",
      "Epoch 506/1000, Training Loss: 0.0262 Time Left: 16.16 seconds\n",
      "Epoch 507/1000, Training Loss: 0.0272 Time Left: 16.13 seconds\n",
      "Epoch 508/1000, Training Loss: 0.0228 Time Left: 16.10 seconds\n",
      "Epoch 509/1000, Training Loss: 0.0259 Time Left: 14.34 seconds\n",
      "Epoch 510/1000, Training Loss: 0.0268 Time Left: 14.31 seconds\n",
      "Epoch 511/1000, Training Loss: 0.0240 Time Left: 14.28 seconds\n",
      "Epoch 512/1000, Training Loss: 0.0242 Time Left: 14.26 seconds\n",
      "Epoch 513/1000, Training Loss: 0.0217 Time Left: 14.23 seconds\n",
      "Epoch 514/1000, Training Loss: 0.0249 Time Left: 14.21 seconds\n",
      "Epoch 515/1000, Training Loss: 0.0349 Time Left: 14.18 seconds\n",
      "Epoch 516/1000, Training Loss: 0.0222 Time Left: 14.16 seconds\n",
      "Epoch 517/1000, Training Loss: 0.0271 Time Left: 14.13 seconds\n",
      "Epoch 518/1000, Training Loss: 0.0586 Time Left: 14.11 seconds\n",
      "Epoch 519/1000, Training Loss: 0.0252 Time Left: 14.09 seconds\n",
      "Epoch 520/1000, Training Loss: 0.0268 Time Left: 14.07 seconds\n",
      "Epoch 521/1000, Training Loss: 0.0292 Time Left: 14.04 seconds\n",
      "Epoch 522/1000, Training Loss: 0.0226 Time Left: 14.02 seconds\n",
      "Epoch 523/1000, Training Loss: 0.0204 Time Left: 13.99 seconds\n",
      "Epoch 524/1000, Training Loss: 0.0239 Time Left: 13.97 seconds\n",
      "Epoch 525/1000, Training Loss: 0.0273 Time Left: 13.94 seconds\n",
      "Epoch 526/1000, Training Loss: 0.0270 Time Left: 13.91 seconds\n",
      "Epoch 527/1000, Training Loss: 0.0262 Time Left: 13.89 seconds\n",
      "Epoch 528/1000, Training Loss: 0.0248 Time Left: 13.87 seconds\n",
      "Epoch 529/1000, Training Loss: 0.0215 Time Left: 13.84 seconds\n",
      "Epoch 530/1000, Training Loss: 0.0265 Time Left: 13.82 seconds\n",
      "Epoch 531/1000, Training Loss: 0.0251 Time Left: 13.80 seconds\n",
      "Epoch 532/1000, Training Loss: 0.0241 Time Left: 13.77 seconds\n",
      "Epoch 533/1000, Training Loss: 0.0243 Time Left: 13.75 seconds\n",
      "Epoch 534/1000, Training Loss: 0.0245 Time Left: 13.73 seconds\n",
      "Epoch 535/1000, Training Loss: 0.0238 Time Left: 13.71 seconds\n",
      "Epoch 536/1000, Training Loss: 0.0195 Time Left: 13.69 seconds\n",
      "Epoch 537/1000, Training Loss: 0.0227 Time Left: 13.67 seconds\n",
      "Epoch 538/1000, Training Loss: 0.0208 Time Left: 13.65 seconds\n",
      "Epoch 539/1000, Training Loss: 0.0181 Time Left: 13.63 seconds\n",
      "Epoch 540/1000, Training Loss: 0.0282 Time Left: 13.61 seconds\n",
      "Epoch 541/1000, Training Loss: 0.0269 Time Left: 13.59 seconds\n",
      "Epoch 542/1000, Training Loss: 0.0202 Time Left: 13.56 seconds\n",
      "Epoch 543/1000, Training Loss: 0.0225 Time Left: 13.54 seconds\n",
      "Epoch 544/1000, Training Loss: 0.0201 Time Left: 13.52 seconds\n",
      "Epoch 545/1000, Training Loss: 0.0238 Time Left: 13.49 seconds\n",
      "Epoch 546/1000, Training Loss: 0.0267 Time Left: 13.47 seconds\n",
      "Epoch 547/1000, Training Loss: 0.0229 Time Left: 13.44 seconds\n",
      "Epoch 548/1000, Training Loss: 0.0217 Time Left: 13.41 seconds\n",
      "Epoch 549/1000, Training Loss: 0.0253 Time Left: 13.38 seconds\n",
      "Epoch 550/1000, Training Loss: 0.0287 Time Left: 13.36 seconds\n",
      "Epoch 551/1000, Training Loss: 0.0216 Time Left: 13.33 seconds\n",
      "Epoch 552/1000, Training Loss: 0.0187 Time Left: 13.30 seconds\n",
      "Epoch 553/1000, Training Loss: 0.0233 Time Left: 13.28 seconds\n",
      "Epoch 554/1000, Training Loss: 0.0226 Time Left: 13.25 seconds\n",
      "Epoch 555/1000, Training Loss: 0.0189 Time Left: 13.22 seconds\n",
      "Epoch 556/1000, Training Loss: 0.0252 Time Left: 13.20 seconds\n",
      "Epoch 557/1000, Training Loss: 0.0268 Time Left: 13.17 seconds\n",
      "Epoch 558/1000, Training Loss: 0.0138 Time Left: 13.14 seconds\n",
      "Epoch 559/1000, Training Loss: 0.0241 Time Left: 13.11 seconds\n",
      "Epoch 560/1000, Training Loss: 0.0261 Time Left: 13.08 seconds\n",
      "Epoch 561/1000, Training Loss: 0.0172 Time Left: 13.05 seconds\n",
      "Epoch 562/1000, Training Loss: 0.0252 Time Left: 13.02 seconds\n",
      "Epoch 563/1000, Training Loss: 0.0181 Time Left: 12.99 seconds\n",
      "Epoch 564/1000, Training Loss: 0.1296 Time Left: 12.97 seconds\n",
      "Epoch 565/1000, Training Loss: 0.0632 Time Left: 12.94 seconds\n",
      "Epoch 566/1000, Training Loss: 0.0376 Time Left: 12.90 seconds\n",
      "Epoch 567/1000, Training Loss: 0.0215 Time Left: 12.87 seconds\n",
      "Epoch 568/1000, Training Loss: 0.0273 Time Left: 12.84 seconds\n",
      "Epoch 569/1000, Training Loss: 0.0231 Time Left: 12.81 seconds\n",
      "Epoch 570/1000, Training Loss: 0.0260 Time Left: 12.78 seconds\n",
      "Epoch 571/1000, Training Loss: 0.0307 Time Left: 12.75 seconds\n",
      "Epoch 572/1000, Training Loss: 0.0341 Time Left: 12.72 seconds\n",
      "Epoch 573/1000, Training Loss: 0.0261 Time Left: 12.68 seconds\n",
      "Epoch 574/1000, Training Loss: 0.0267 Time Left: 12.65 seconds\n",
      "Epoch 575/1000, Training Loss: 0.0265 Time Left: 12.62 seconds\n",
      "Epoch 576/1000, Training Loss: 0.0264 Time Left: 12.58 seconds\n",
      "Epoch 577/1000, Training Loss: 0.0261 Time Left: 12.55 seconds\n",
      "Epoch 578/1000, Training Loss: 0.0237 Time Left: 12.52 seconds\n",
      "Epoch 579/1000, Training Loss: 0.0320 Time Left: 12.49 seconds\n",
      "Epoch 580/1000, Training Loss: 0.0232 Time Left: 12.46 seconds\n",
      "Epoch 581/1000, Training Loss: 0.0288 Time Left: 12.43 seconds\n",
      "Epoch 582/1000, Training Loss: 0.0295 Time Left: 12.40 seconds\n",
      "Epoch 583/1000, Training Loss: 0.0208 Time Left: 12.37 seconds\n",
      "Epoch 584/1000, Training Loss: 0.0208 Time Left: 12.33 seconds\n",
      "Epoch 585/1000, Training Loss: 0.0292 Time Left: 12.30 seconds\n",
      "Epoch 586/1000, Training Loss: 0.0289 Time Left: 12.27 seconds\n",
      "Epoch 587/1000, Training Loss: 0.0163 Time Left: 12.24 seconds\n",
      "Epoch 588/1000, Training Loss: 0.0279 Time Left: 12.21 seconds\n",
      "Epoch 589/1000, Training Loss: 0.0191 Time Left: 12.18 seconds\n",
      "Epoch 590/1000, Training Loss: 0.0192 Time Left: 12.15 seconds\n",
      "Epoch 591/1000, Training Loss: 0.0196 Time Left: 12.11 seconds\n",
      "Epoch 592/1000, Training Loss: 0.0320 Time Left: 12.08 seconds\n",
      "Epoch 593/1000, Training Loss: 0.0176 Time Left: 12.05 seconds\n",
      "Epoch 594/1000, Training Loss: 0.0194 Time Left: 12.02 seconds\n",
      "Epoch 595/1000, Training Loss: 0.0229 Time Left: 11.99 seconds\n",
      "Epoch 596/1000, Training Loss: 0.0235 Time Left: 11.96 seconds\n",
      "Epoch 597/1000, Training Loss: 0.0262 Time Left: 11.93 seconds\n",
      "Epoch 598/1000, Training Loss: 0.0283 Time Left: 11.90 seconds\n",
      "Epoch 599/1000, Training Loss: 0.0291 Time Left: 11.87 seconds\n",
      "Epoch 600/1000, Validation Loss: 0.0759\n",
      "Epoch 600/1000, Training Loss: 0.0247 Time Left: 11.85 seconds\n",
      "Epoch 601/1000, Training Loss: 0.0207 Time Left: 11.82 seconds\n",
      "Epoch 602/1000, Training Loss: 0.0187 Time Left: 11.79 seconds\n",
      "Epoch 603/1000, Training Loss: 0.0181 Time Left: 11.76 seconds\n",
      "Epoch 604/1000, Training Loss: 0.0242 Time Left: 11.73 seconds\n",
      "Epoch 605/1000, Training Loss: 0.0244 Time Left: 11.70 seconds\n",
      "Epoch 606/1000, Training Loss: 0.0286 Time Left: 11.67 seconds\n",
      "Epoch 607/1000, Training Loss: 0.0215 Time Left: 11.64 seconds\n",
      "Epoch 608/1000, Training Loss: 0.0256 Time Left: 11.61 seconds\n",
      "Epoch 609/1000, Training Loss: 0.0263 Time Left: 11.58 seconds\n",
      "Epoch 610/1000, Training Loss: 0.0224 Time Left: 11.56 seconds\n",
      "Epoch 611/1000, Training Loss: 0.0282 Time Left: 11.53 seconds\n",
      "Epoch 612/1000, Training Loss: 0.0225 Time Left: 11.50 seconds\n",
      "Epoch 613/1000, Training Loss: 0.0243 Time Left: 11.47 seconds\n",
      "Epoch 614/1000, Training Loss: 0.0203 Time Left: 11.44 seconds\n",
      "Epoch 615/1000, Training Loss: 0.0307 Time Left: 11.41 seconds\n",
      "Epoch 616/1000, Training Loss: 0.0251 Time Left: 11.38 seconds\n",
      "Epoch 617/1000, Training Loss: 0.0254 Time Left: 11.35 seconds\n",
      "Epoch 618/1000, Training Loss: 0.0297 Time Left: 11.32 seconds\n",
      "Epoch 619/1000, Training Loss: 0.0194 Time Left: 11.29 seconds\n",
      "Epoch 620/1000, Training Loss: 0.0171 Time Left: 11.27 seconds\n",
      "Epoch 621/1000, Training Loss: 0.0352 Time Left: 11.24 seconds\n",
      "Epoch 622/1000, Training Loss: 0.0266 Time Left: 11.21 seconds\n",
      "Epoch 623/1000, Training Loss: 0.0279 Time Left: 11.18 seconds\n",
      "Epoch 624/1000, Training Loss: 0.0248 Time Left: 11.15 seconds\n",
      "Epoch 625/1000, Training Loss: 0.0241 Time Left: 11.12 seconds\n",
      "Epoch 626/1000, Training Loss: 0.0237 Time Left: 11.09 seconds\n",
      "Epoch 627/1000, Training Loss: 0.0266 Time Left: 11.07 seconds\n",
      "Epoch 628/1000, Training Loss: 0.0213 Time Left: 11.04 seconds\n",
      "Epoch 629/1000, Training Loss: 0.0271 Time Left: 11.01 seconds\n",
      "Epoch 630/1000, Training Loss: 0.0282 Time Left: 10.98 seconds\n",
      "Epoch 631/1000, Training Loss: 0.0241 Time Left: 10.95 seconds\n",
      "Epoch 632/1000, Training Loss: 0.0256 Time Left: 10.92 seconds\n",
      "Epoch 633/1000, Training Loss: 0.0204 Time Left: 10.89 seconds\n",
      "Epoch 634/1000, Training Loss: 0.0234 Time Left: 10.86 seconds\n",
      "Epoch 635/1000, Training Loss: 0.0164 Time Left: 10.83 seconds\n",
      "Epoch 636/1000, Training Loss: 0.0241 Time Left: 10.80 seconds\n",
      "Epoch 637/1000, Training Loss: 0.0321 Time Left: 10.78 seconds\n",
      "Epoch 638/1000, Training Loss: 0.0194 Time Left: 10.75 seconds\n",
      "Epoch 639/1000, Training Loss: 0.0278 Time Left: 10.72 seconds\n",
      "Epoch 640/1000, Training Loss: 0.0226 Time Left: 10.69 seconds\n",
      "Epoch 641/1000, Training Loss: 0.0263 Time Left: 10.66 seconds\n",
      "Epoch 642/1000, Training Loss: 0.0235 Time Left: 10.63 seconds\n",
      "Epoch 643/1000, Training Loss: 0.0241 Time Left: 10.60 seconds\n",
      "Epoch 644/1000, Training Loss: 0.0211 Time Left: 10.58 seconds\n",
      "Epoch 645/1000, Training Loss: 0.0229 Time Left: 10.55 seconds\n",
      "Epoch 646/1000, Training Loss: 0.0122 Time Left: 10.52 seconds\n",
      "Epoch 647/1000, Training Loss: 0.0278 Time Left: 10.49 seconds\n",
      "Epoch 648/1000, Training Loss: 0.0269 Time Left: 10.47 seconds\n",
      "Epoch 649/1000, Training Loss: 0.0245 Time Left: 10.44 seconds\n",
      "Epoch 650/1000, Training Loss: 0.0177 Time Left: 10.41 seconds\n",
      "Epoch 651/1000, Training Loss: 0.0251 Time Left: 10.38 seconds\n",
      "Epoch 652/1000, Training Loss: 0.0258 Time Left: 10.36 seconds\n",
      "Epoch 653/1000, Training Loss: 0.0222 Time Left: 10.33 seconds\n",
      "Epoch 654/1000, Training Loss: 0.0242 Time Left: 10.30 seconds\n",
      "Epoch 655/1000, Training Loss: 0.0173 Time Left: 10.27 seconds\n",
      "Epoch 656/1000, Training Loss: 0.0199 Time Left: 10.25 seconds\n",
      "Epoch 657/1000, Training Loss: 0.0356 Time Left: 10.22 seconds\n",
      "Epoch 658/1000, Training Loss: 0.0191 Time Left: 10.19 seconds\n",
      "Epoch 659/1000, Training Loss: 0.0194 Time Left: 10.16 seconds\n",
      "Epoch 660/1000, Training Loss: 0.0243 Time Left: 10.13 seconds\n",
      "Epoch 661/1000, Training Loss: 0.0184 Time Left: 10.11 seconds\n",
      "Epoch 662/1000, Training Loss: 0.0284 Time Left: 10.08 seconds\n",
      "Epoch 663/1000, Training Loss: 0.0228 Time Left: 10.05 seconds\n",
      "Epoch 664/1000, Training Loss: 0.0254 Time Left: 10.02 seconds\n",
      "Epoch 665/1000, Training Loss: 0.0096 Time Left: 9.99 seconds\n",
      "Epoch 666/1000, Training Loss: 0.0283 Time Left: 9.96 seconds\n",
      "Epoch 667/1000, Training Loss: 0.0329 Time Left: 9.94 seconds\n",
      "Epoch 668/1000, Training Loss: 0.0118 Time Left: 9.91 seconds\n",
      "Epoch 669/1000, Training Loss: 0.0203 Time Left: 9.88 seconds\n",
      "Epoch 670/1000, Training Loss: 0.0261 Time Left: 9.85 seconds\n",
      "Epoch 671/1000, Training Loss: 0.2359 Time Left: 9.82 seconds\n",
      "Epoch 672/1000, Training Loss: 0.0239 Time Left: 9.80 seconds\n",
      "Epoch 673/1000, Training Loss: 0.0373 Time Left: 9.77 seconds\n",
      "Epoch 674/1000, Training Loss: 0.0091 Time Left: 9.74 seconds\n",
      "Epoch 675/1000, Training Loss: 0.0221 Time Left: 9.71 seconds\n",
      "Epoch 676/1000, Training Loss: 0.0222 Time Left: 9.68 seconds\n",
      "Epoch 677/1000, Training Loss: 0.0318 Time Left: 9.66 seconds\n",
      "Epoch 678/1000, Training Loss: 0.0258 Time Left: 9.63 seconds\n",
      "Epoch 679/1000, Training Loss: 0.0242 Time Left: 9.60 seconds\n",
      "Epoch 680/1000, Training Loss: 0.0227 Time Left: 9.57 seconds\n",
      "Epoch 681/1000, Training Loss: 0.0244 Time Left: 9.54 seconds\n",
      "Epoch 682/1000, Training Loss: 0.0237 Time Left: 9.51 seconds\n",
      "Epoch 683/1000, Training Loss: 0.0295 Time Left: 9.49 seconds\n",
      "Epoch 684/1000, Training Loss: 0.0432 Time Left: 9.46 seconds\n",
      "Epoch 685/1000, Training Loss: 0.0285 Time Left: 9.43 seconds\n",
      "Epoch 686/1000, Training Loss: 0.0282 Time Left: 9.40 seconds\n",
      "Epoch 687/1000, Training Loss: 0.0246 Time Left: 9.37 seconds\n",
      "Epoch 688/1000, Training Loss: 0.0191 Time Left: 9.34 seconds\n",
      "Epoch 689/1000, Training Loss: 0.0067 Time Left: 9.32 seconds\n",
      "Epoch 690/1000, Training Loss: 0.0185 Time Left: 9.29 seconds\n",
      "Epoch 691/1000, Training Loss: 0.0230 Time Left: 9.26 seconds\n",
      "Epoch 692/1000, Training Loss: 0.0247 Time Left: 9.23 seconds\n",
      "Epoch 693/1000, Training Loss: 0.0255 Time Left: 9.21 seconds\n",
      "Epoch 694/1000, Training Loss: 0.0231 Time Left: 9.18 seconds\n",
      "Epoch 695/1000, Training Loss: 0.0242 Time Left: 9.15 seconds\n",
      "Epoch 696/1000, Training Loss: 0.0210 Time Left: 9.12 seconds\n",
      "Epoch 697/1000, Training Loss: 0.0183 Time Left: 9.09 seconds\n",
      "Epoch 698/1000, Training Loss: 0.0238 Time Left: 9.06 seconds\n",
      "Epoch 699/1000, Training Loss: 0.0497 Time Left: 9.03 seconds\n",
      "Epoch 700/1000, Validation Loss: 0.0710\n",
      "Epoch 700/1000, Training Loss: 0.0270 Time Left: 9.01 seconds\n",
      "Epoch 701/1000, Training Loss: 0.0253 Time Left: 8.98 seconds\n",
      "Epoch 702/1000, Training Loss: 0.0273 Time Left: 8.95 seconds\n",
      "Epoch 703/1000, Training Loss: 0.0257 Time Left: 8.92 seconds\n",
      "Epoch 704/1000, Training Loss: 0.0253 Time Left: 8.89 seconds\n",
      "Epoch 705/1000, Training Loss: 0.0242 Time Left: 8.86 seconds\n",
      "Epoch 706/1000, Training Loss: 0.0206 Time Left: 8.83 seconds\n",
      "Epoch 707/1000, Training Loss: 0.0212 Time Left: 8.81 seconds\n",
      "Epoch 708/1000, Training Loss: 0.0216 Time Left: 8.78 seconds\n",
      "Epoch 709/1000, Training Loss: 0.0278 Time Left: 8.75 seconds\n",
      "Epoch 710/1000, Training Loss: 0.0225 Time Left: 8.72 seconds\n",
      "Epoch 711/1000, Training Loss: 0.0250 Time Left: 8.69 seconds\n",
      "Epoch 712/1000, Training Loss: 0.0267 Time Left: 8.66 seconds\n",
      "Epoch 713/1000, Training Loss: 0.0238 Time Left: 8.64 seconds\n",
      "Epoch 714/1000, Training Loss: 0.0225 Time Left: 8.61 seconds\n",
      "Epoch 715/1000, Training Loss: 0.0266 Time Left: 8.58 seconds\n",
      "Epoch 716/1000, Training Loss: 0.0187 Time Left: 8.55 seconds\n",
      "Epoch 717/1000, Training Loss: 0.0200 Time Left: 8.52 seconds\n",
      "Epoch 718/1000, Training Loss: 0.0269 Time Left: 8.49 seconds\n",
      "Epoch 719/1000, Training Loss: 0.0173 Time Left: 8.46 seconds\n",
      "Epoch 720/1000, Training Loss: 0.0227 Time Left: 8.43 seconds\n",
      "Epoch 721/1000, Training Loss: 7.8132 Time Left: 8.41 seconds\n",
      "Epoch 722/1000, Training Loss: 0.0364 Time Left: 8.38 seconds\n",
      "Epoch 723/1000, Training Loss: 0.0275 Time Left: 8.35 seconds\n",
      "Epoch 724/1000, Training Loss: 0.0287 Time Left: 8.32 seconds\n",
      "Epoch 725/1000, Training Loss: 0.0198 Time Left: 8.29 seconds\n",
      "Epoch 726/1000, Training Loss: 0.0368 Time Left: 8.26 seconds\n",
      "Epoch 727/1000, Training Loss: 0.0286 Time Left: 8.23 seconds\n",
      "Epoch 728/1000, Training Loss: 0.0280 Time Left: 8.20 seconds\n",
      "Epoch 729/1000, Training Loss: 0.0263 Time Left: 8.17 seconds\n",
      "Epoch 730/1000, Training Loss: 0.0296 Time Left: 8.15 seconds\n",
      "Epoch 731/1000, Training Loss: 0.0238 Time Left: 8.12 seconds\n",
      "Epoch 732/1000, Training Loss: 0.0247 Time Left: 8.09 seconds\n",
      "Epoch 733/1000, Training Loss: 0.0286 Time Left: 8.06 seconds\n",
      "Epoch 734/1000, Training Loss: 0.0202 Time Left: 8.03 seconds\n",
      "Epoch 735/1000, Training Loss: 0.0222 Time Left: 8.00 seconds\n",
      "Epoch 736/1000, Training Loss: 0.0236 Time Left: 7.97 seconds\n",
      "Epoch 737/1000, Training Loss: 0.0287 Time Left: 7.94 seconds\n",
      "Epoch 738/1000, Training Loss: 0.0258 Time Left: 7.91 seconds\n",
      "Epoch 739/1000, Training Loss: 0.0364 Time Left: 7.89 seconds\n",
      "Epoch 740/1000, Training Loss: 0.0248 Time Left: 7.86 seconds\n",
      "Epoch 741/1000, Training Loss: 0.0225 Time Left: 7.83 seconds\n",
      "Epoch 742/1000, Training Loss: 0.0246 Time Left: 7.80 seconds\n",
      "Epoch 743/1000, Training Loss: 0.0264 Time Left: 7.77 seconds\n",
      "Epoch 744/1000, Training Loss: 0.0298 Time Left: 7.74 seconds\n",
      "Epoch 745/1000, Training Loss: 0.0198 Time Left: 7.71 seconds\n",
      "Epoch 746/1000, Training Loss: 0.0248 Time Left: 7.68 seconds\n",
      "Epoch 747/1000, Training Loss: 0.0298 Time Left: 7.65 seconds\n",
      "Epoch 748/1000, Training Loss: 0.0288 Time Left: 7.62 seconds\n",
      "Epoch 749/1000, Training Loss: 0.0199 Time Left: 7.59 seconds\n",
      "Epoch 750/1000, Training Loss: 0.0171 Time Left: 7.57 seconds\n",
      "Epoch 751/1000, Training Loss: 0.0232 Time Left: 7.54 seconds\n",
      "Epoch 752/1000, Training Loss: 0.0325 Time Left: 7.51 seconds\n",
      "Epoch 753/1000, Training Loss: 0.0206 Time Left: 7.48 seconds\n",
      "Epoch 754/1000, Training Loss: 0.0270 Time Left: 7.45 seconds\n",
      "Epoch 755/1000, Training Loss: 0.0224 Time Left: 7.42 seconds\n",
      "Epoch 756/1000, Training Loss: 0.0093 Time Left: 7.40 seconds\n",
      "Epoch 757/1000, Training Loss: 0.0270 Time Left: 7.37 seconds\n",
      "Epoch 758/1000, Training Loss: 0.0303 Time Left: 7.34 seconds\n",
      "Epoch 759/1000, Training Loss: 0.0305 Time Left: 7.31 seconds\n",
      "Epoch 760/1000, Training Loss: 0.0290 Time Left: 7.28 seconds\n",
      "Epoch 761/1000, Training Loss: 0.0284 Time Left: 7.25 seconds\n",
      "Epoch 762/1000, Training Loss: 0.0221 Time Left: 7.22 seconds\n",
      "Epoch 763/1000, Training Loss: 0.0178 Time Left: 7.19 seconds\n",
      "Epoch 764/1000, Training Loss: 0.0322 Time Left: 7.16 seconds\n",
      "Epoch 765/1000, Training Loss: 0.0253 Time Left: 7.14 seconds\n",
      "Epoch 766/1000, Training Loss: 0.0189 Time Left: 7.11 seconds\n",
      "Epoch 767/1000, Training Loss: 0.0268 Time Left: 7.08 seconds\n",
      "Epoch 768/1000, Training Loss: 0.0235 Time Left: 7.05 seconds\n",
      "Epoch 769/1000, Training Loss: 0.0293 Time Left: 7.02 seconds\n",
      "Epoch 770/1000, Training Loss: 0.0280 Time Left: 6.99 seconds\n",
      "Epoch 771/1000, Training Loss: 0.0282 Time Left: 6.96 seconds\n",
      "Epoch 772/1000, Training Loss: 0.0252 Time Left: 6.93 seconds\n",
      "Epoch 773/1000, Training Loss: 0.0235 Time Left: 6.90 seconds\n",
      "Epoch 774/1000, Training Loss: 0.0235 Time Left: 6.88 seconds\n",
      "Epoch 775/1000, Training Loss: 0.0251 Time Left: 6.85 seconds\n",
      "Epoch 776/1000, Training Loss: 0.0244 Time Left: 6.82 seconds\n",
      "Epoch 777/1000, Training Loss: 0.0241 Time Left: 6.79 seconds\n",
      "Epoch 778/1000, Training Loss: 0.0250 Time Left: 6.76 seconds\n",
      "Epoch 779/1000, Training Loss: 0.0170 Time Left: 6.73 seconds\n",
      "Epoch 780/1000, Training Loss: 0.0188 Time Left: 6.70 seconds\n",
      "Epoch 781/1000, Training Loss: 0.0211 Time Left: 6.67 seconds\n",
      "Epoch 782/1000, Training Loss: 0.0237 Time Left: 6.64 seconds\n",
      "Epoch 783/1000, Training Loss: 0.0236 Time Left: 6.61 seconds\n",
      "Epoch 784/1000, Training Loss: 0.0201 Time Left: 6.58 seconds\n",
      "Epoch 785/1000, Training Loss: 0.0197 Time Left: 6.55 seconds\n",
      "Epoch 786/1000, Training Loss: 0.0238 Time Left: 6.52 seconds\n",
      "Epoch 787/1000, Training Loss: 0.0208 Time Left: 6.50 seconds\n",
      "Epoch 788/1000, Training Loss: 0.0259 Time Left: 6.47 seconds\n",
      "Epoch 789/1000, Training Loss: 0.0180 Time Left: 6.44 seconds\n",
      "Epoch 790/1000, Training Loss: 0.0256 Time Left: 6.41 seconds\n",
      "Epoch 791/1000, Training Loss: 0.0214 Time Left: 6.38 seconds\n",
      "Epoch 792/1000, Training Loss: 0.0197 Time Left: 6.35 seconds\n",
      "Epoch 793/1000, Training Loss: -0.0314 Time Left: 6.32 seconds\n",
      "Epoch 794/1000, Training Loss: 0.0182 Time Left: 6.29 seconds\n",
      "Epoch 795/1000, Training Loss: 0.0410 Time Left: 6.26 seconds\n",
      "Epoch 796/1000, Training Loss: 0.0224 Time Left: 6.23 seconds\n",
      "Epoch 797/1000, Training Loss: 0.0267 Time Left: 6.20 seconds\n",
      "Epoch 798/1000, Training Loss: 0.0238 Time Left: 6.17 seconds\n",
      "Epoch 799/1000, Training Loss: 0.0126 Time Left: 6.14 seconds\n",
      "Epoch 800/1000, Validation Loss: 0.0745\n",
      "Epoch 800/1000, Training Loss: 0.0255 Time Left: 6.11 seconds\n",
      "Epoch 801/1000, Training Loss: 0.0184 Time Left: 6.08 seconds\n",
      "Epoch 802/1000, Training Loss: 0.0257 Time Left: 6.06 seconds\n",
      "Epoch 803/1000, Training Loss: 0.0236 Time Left: 6.03 seconds\n",
      "Epoch 804/1000, Training Loss: 0.0212 Time Left: 5.99 seconds\n",
      "Epoch 805/1000, Training Loss: 0.0177 Time Left: 5.96 seconds\n",
      "Epoch 806/1000, Training Loss: 0.0248 Time Left: 5.93 seconds\n",
      "Epoch 807/1000, Training Loss: 0.0259 Time Left: 5.91 seconds\n",
      "Epoch 808/1000, Training Loss: 0.0145 Time Left: 5.87 seconds\n",
      "Epoch 809/1000, Training Loss: 0.0202 Time Left: 5.85 seconds\n",
      "Epoch 810/1000, Training Loss: 0.0164 Time Left: 5.82 seconds\n",
      "Epoch 811/1000, Training Loss: 0.0122 Time Left: 5.79 seconds\n",
      "Epoch 812/1000, Training Loss: 0.8337 Time Left: 5.76 seconds\n",
      "Epoch 813/1000, Training Loss: 0.0465 Time Left: 5.73 seconds\n",
      "Epoch 814/1000, Training Loss: 0.0292 Time Left: 5.70 seconds\n",
      "Epoch 815/1000, Training Loss: 0.0369 Time Left: 5.66 seconds\n",
      "Epoch 816/1000, Training Loss: 0.0297 Time Left: 5.63 seconds\n",
      "Epoch 817/1000, Training Loss: 0.0323 Time Left: 5.60 seconds\n",
      "Epoch 818/1000, Training Loss: 0.2142 Time Left: 5.57 seconds\n",
      "Epoch 819/1000, Training Loss: 0.0280 Time Left: 5.54 seconds\n",
      "Epoch 820/1000, Training Loss: 0.0338 Time Left: 5.51 seconds\n",
      "Epoch 821/1000, Training Loss: 0.0291 Time Left: 5.48 seconds\n",
      "Epoch 822/1000, Training Loss: 0.0293 Time Left: 5.45 seconds\n",
      "Epoch 823/1000, Training Loss: 0.0297 Time Left: 5.42 seconds\n",
      "Epoch 824/1000, Training Loss: 0.0291 Time Left: 5.39 seconds\n",
      "Epoch 825/1000, Training Loss: 0.0261 Time Left: 5.36 seconds\n",
      "Epoch 826/1000, Training Loss: 0.0271 Time Left: 5.33 seconds\n",
      "Epoch 827/1000, Training Loss: 0.0220 Time Left: 5.30 seconds\n",
      "Epoch 828/1000, Training Loss: 0.0201 Time Left: 5.27 seconds\n",
      "Epoch 829/1000, Training Loss: 0.0297 Time Left: 5.24 seconds\n",
      "Epoch 830/1000, Training Loss: 0.0234 Time Left: 5.21 seconds\n",
      "Epoch 831/1000, Training Loss: 0.0226 Time Left: 5.18 seconds\n",
      "Epoch 832/1000, Training Loss: 0.0223 Time Left: 5.15 seconds\n",
      "Epoch 833/1000, Training Loss: 0.0241 Time Left: 5.12 seconds\n",
      "Epoch 834/1000, Training Loss: 0.0258 Time Left: 5.08 seconds\n",
      "Epoch 835/1000, Training Loss: 0.0236 Time Left: 5.05 seconds\n",
      "Epoch 836/1000, Training Loss: 0.0248 Time Left: 5.02 seconds\n",
      "Epoch 837/1000, Training Loss: 0.0214 Time Left: 4.99 seconds\n",
      "Epoch 838/1000, Training Loss: 0.0322 Time Left: 4.96 seconds\n",
      "Epoch 839/1000, Training Loss: 0.0235 Time Left: 4.93 seconds\n",
      "Epoch 840/1000, Training Loss: 0.0220 Time Left: 4.90 seconds\n",
      "Epoch 841/1000, Training Loss: 0.0261 Time Left: 4.87 seconds\n",
      "Epoch 842/1000, Training Loss: 0.0239 Time Left: 4.84 seconds\n",
      "Epoch 843/1000, Training Loss: 0.0216 Time Left: 4.81 seconds\n",
      "Epoch 844/1000, Training Loss: 0.0253 Time Left: 4.78 seconds\n",
      "Epoch 845/1000, Training Loss: 0.0232 Time Left: 4.75 seconds\n",
      "Epoch 846/1000, Training Loss: 0.0206 Time Left: 4.72 seconds\n",
      "Epoch 847/1000, Training Loss: 0.0256 Time Left: 4.69 seconds\n",
      "Epoch 848/1000, Training Loss: 0.0178 Time Left: 4.66 seconds\n",
      "Epoch 849/1000, Training Loss: 0.0271 Time Left: 4.63 seconds\n",
      "Epoch 850/1000, Training Loss: 0.0225 Time Left: 4.60 seconds\n",
      "Epoch 851/1000, Training Loss: 0.0308 Time Left: 4.57 seconds\n",
      "Epoch 852/1000, Training Loss: 0.0253 Time Left: 4.54 seconds\n",
      "Epoch 853/1000, Training Loss: 0.0210 Time Left: 4.51 seconds\n",
      "Epoch 854/1000, Training Loss: 0.0266 Time Left: 4.48 seconds\n",
      "Epoch 855/1000, Training Loss: 0.0247 Time Left: 4.44 seconds\n",
      "Epoch 856/1000, Training Loss: 0.0202 Time Left: 4.41 seconds\n",
      "Epoch 857/1000, Training Loss: 0.0210 Time Left: 4.38 seconds\n",
      "Epoch 858/1000, Training Loss: 0.0271 Time Left: 4.35 seconds\n",
      "Epoch 859/1000, Training Loss: 0.0325 Time Left: 4.32 seconds\n",
      "Epoch 860/1000, Training Loss: 0.0190 Time Left: 4.29 seconds\n",
      "Epoch 861/1000, Training Loss: 0.0213 Time Left: 4.26 seconds\n",
      "Epoch 862/1000, Training Loss: 0.0259 Time Left: 4.23 seconds\n",
      "Epoch 863/1000, Training Loss: 0.0241 Time Left: 4.20 seconds\n",
      "Epoch 864/1000, Training Loss: 0.0258 Time Left: 4.17 seconds\n",
      "Epoch 865/1000, Training Loss: 0.0227 Time Left: 4.14 seconds\n",
      "Epoch 866/1000, Training Loss: 0.0221 Time Left: 4.11 seconds\n",
      "Epoch 867/1000, Training Loss: 0.0263 Time Left: 4.08 seconds\n",
      "Epoch 868/1000, Training Loss: 0.0276 Time Left: 4.05 seconds\n",
      "Epoch 869/1000, Training Loss: 0.0199 Time Left: 4.02 seconds\n",
      "Epoch 870/1000, Training Loss: 0.0228 Time Left: 3.99 seconds\n",
      "Epoch 871/1000, Training Loss: 0.0278 Time Left: 3.96 seconds\n",
      "Epoch 872/1000, Training Loss: 0.0277 Time Left: 3.93 seconds\n",
      "Epoch 873/1000, Training Loss: 0.0283 Time Left: 3.89 seconds\n",
      "Epoch 874/1000, Training Loss: 0.0196 Time Left: 3.86 seconds\n",
      "Epoch 875/1000, Training Loss: 0.0240 Time Left: 3.83 seconds\n",
      "Epoch 876/1000, Training Loss: 0.0187 Time Left: 3.80 seconds\n",
      "Epoch 877/1000, Training Loss: 0.0288 Time Left: 3.77 seconds\n",
      "Epoch 878/1000, Training Loss: 0.0139 Time Left: 3.74 seconds\n",
      "Epoch 879/1000, Training Loss: 0.0249 Time Left: 3.71 seconds\n",
      "Epoch 880/1000, Training Loss: 0.0299 Time Left: 3.68 seconds\n",
      "Epoch 881/1000, Training Loss: 0.0211 Time Left: 3.65 seconds\n",
      "Epoch 882/1000, Training Loss: 0.0245 Time Left: 3.62 seconds\n",
      "Epoch 883/1000, Training Loss: 0.0175 Time Left: 3.59 seconds\n",
      "Epoch 884/1000, Training Loss: 0.0216 Time Left: 3.56 seconds\n",
      "Epoch 885/1000, Training Loss: 0.0241 Time Left: 3.53 seconds\n",
      "Epoch 886/1000, Training Loss: 0.0216 Time Left: 3.50 seconds\n",
      "Epoch 887/1000, Training Loss: 0.0271 Time Left: 3.47 seconds\n",
      "Epoch 888/1000, Training Loss: 0.0254 Time Left: 3.44 seconds\n",
      "Epoch 889/1000, Training Loss: 0.0201 Time Left: 3.41 seconds\n",
      "Epoch 890/1000, Training Loss: 0.0258 Time Left: 3.38 seconds\n",
      "Epoch 891/1000, Training Loss: 0.0228 Time Left: 3.34 seconds\n",
      "Epoch 892/1000, Training Loss: 0.0217 Time Left: 3.31 seconds\n",
      "Epoch 893/1000, Training Loss: 0.0292 Time Left: 3.28 seconds\n",
      "Epoch 894/1000, Training Loss: 0.0191 Time Left: 3.25 seconds\n",
      "Epoch 895/1000, Training Loss: 0.0241 Time Left: 3.22 seconds\n",
      "Epoch 896/1000, Training Loss: 0.0226 Time Left: 3.19 seconds\n",
      "Epoch 897/1000, Training Loss: 0.0226 Time Left: 3.16 seconds\n",
      "Epoch 898/1000, Training Loss: 0.0174 Time Left: 3.13 seconds\n",
      "Epoch 899/1000, Training Loss: 0.0226 Time Left: 3.10 seconds\n",
      "Epoch 900/1000, Validation Loss: 0.0849\n",
      "Epoch 900/1000, Training Loss: 0.0209 Time Left: 3.07 seconds\n",
      "Epoch 901/1000, Training Loss: 0.0291 Time Left: 3.04 seconds\n",
      "Epoch 902/1000, Training Loss: 0.0210 Time Left: 3.01 seconds\n",
      "Epoch 903/1000, Training Loss: 0.0316 Time Left: 2.98 seconds\n",
      "Epoch 904/1000, Training Loss: 0.0240 Time Left: 2.95 seconds\n",
      "Epoch 905/1000, Training Loss: 0.0270 Time Left: 2.92 seconds\n",
      "Epoch 906/1000, Training Loss: 0.0249 Time Left: 2.89 seconds\n",
      "Epoch 907/1000, Training Loss: 0.0240 Time Left: 2.86 seconds\n",
      "Epoch 908/1000, Training Loss: 0.0234 Time Left: 2.83 seconds\n",
      "Epoch 909/1000, Training Loss: 0.0226 Time Left: 2.80 seconds\n",
      "Epoch 910/1000, Training Loss: 0.0307 Time Left: 2.77 seconds\n",
      "Epoch 911/1000, Training Loss: 0.0253 Time Left: 2.74 seconds\n",
      "Epoch 912/1000, Training Loss: 0.0239 Time Left: 2.71 seconds\n",
      "Epoch 913/1000, Training Loss: 0.0203 Time Left: 2.68 seconds\n",
      "Epoch 914/1000, Training Loss: 0.0237 Time Left: 2.64 seconds\n",
      "Epoch 915/1000, Training Loss: 0.0188 Time Left: 2.61 seconds\n",
      "Epoch 916/1000, Training Loss: 0.0268 Time Left: 2.58 seconds\n",
      "Epoch 917/1000, Training Loss: 0.0252 Time Left: 2.55 seconds\n",
      "Epoch 918/1000, Training Loss: 0.0252 Time Left: 2.52 seconds\n",
      "Epoch 919/1000, Training Loss: 0.0197 Time Left: 2.49 seconds\n",
      "Epoch 920/1000, Training Loss: 0.0252 Time Left: 2.46 seconds\n",
      "Epoch 921/1000, Training Loss: 0.0283 Time Left: 2.43 seconds\n",
      "Epoch 922/1000, Training Loss: 0.0216 Time Left: 2.40 seconds\n",
      "Epoch 923/1000, Training Loss: 0.0266 Time Left: 2.37 seconds\n",
      "Epoch 924/1000, Training Loss: 0.0246 Time Left: 2.34 seconds\n",
      "Epoch 925/1000, Training Loss: 0.0203 Time Left: 2.31 seconds\n",
      "Epoch 926/1000, Training Loss: 0.0203 Time Left: 2.28 seconds\n",
      "Epoch 927/1000, Training Loss: 0.0183 Time Left: 2.25 seconds\n",
      "Epoch 928/1000, Training Loss: 0.0197 Time Left: 2.22 seconds\n",
      "Epoch 929/1000, Training Loss: 0.0233 Time Left: 2.19 seconds\n",
      "Epoch 930/1000, Training Loss: 0.0216 Time Left: 2.16 seconds\n",
      "Epoch 931/1000, Training Loss: 0.0237 Time Left: 2.13 seconds\n",
      "Epoch 932/1000, Training Loss: 0.0241 Time Left: 2.09 seconds\n",
      "Epoch 933/1000, Training Loss: 0.0202 Time Left: 2.07 seconds\n",
      "Epoch 934/1000, Training Loss: 0.0231 Time Left: 2.03 seconds\n",
      "Epoch 935/1000, Training Loss: 0.0232 Time Left: 2.00 seconds\n",
      "Epoch 936/1000, Training Loss: 0.0222 Time Left: 1.97 seconds\n",
      "Epoch 937/1000, Training Loss: 0.0210 Time Left: 1.94 seconds\n",
      "Epoch 938/1000, Training Loss: 0.0222 Time Left: 1.91 seconds\n",
      "Epoch 939/1000, Training Loss: 0.0250 Time Left: 1.88 seconds\n",
      "Epoch 940/1000, Training Loss: 0.0240 Time Left: 1.85 seconds\n",
      "Epoch 941/1000, Training Loss: 0.0196 Time Left: 1.82 seconds\n",
      "Epoch 942/1000, Training Loss: 0.0177 Time Left: 1.79 seconds\n",
      "Epoch 943/1000, Training Loss: 0.0217 Time Left: 1.76 seconds\n",
      "Epoch 944/1000, Training Loss: 0.0205 Time Left: 1.73 seconds\n",
      "Epoch 945/1000, Training Loss: 0.0295 Time Left: 1.70 seconds\n",
      "Epoch 946/1000, Training Loss: 0.0151 Time Left: 1.67 seconds\n",
      "Epoch 947/1000, Training Loss: 0.0282 Time Left: 1.64 seconds\n",
      "Epoch 948/1000, Training Loss: 0.0212 Time Left: 1.61 seconds\n",
      "Epoch 949/1000, Training Loss: 0.0250 Time Left: 1.57 seconds\n",
      "Epoch 950/1000, Training Loss: 0.0276 Time Left: 1.54 seconds\n",
      "Epoch 951/1000, Training Loss: 0.0159 Time Left: 1.51 seconds\n",
      "Epoch 952/1000, Training Loss: 0.0217 Time Left: 1.48 seconds\n",
      "Epoch 953/1000, Training Loss: 0.0223 Time Left: 1.45 seconds\n",
      "Epoch 954/1000, Training Loss: 0.0289 Time Left: 1.42 seconds\n",
      "Epoch 955/1000, Training Loss: 0.0181 Time Left: 1.39 seconds\n",
      "Epoch 956/1000, Training Loss: 0.0278 Time Left: 1.36 seconds\n",
      "Epoch 957/1000, Training Loss: 0.0233 Time Left: 1.33 seconds\n",
      "Epoch 958/1000, Training Loss: 0.0230 Time Left: 1.30 seconds\n",
      "Epoch 959/1000, Training Loss: 0.0225 Time Left: 1.27 seconds\n",
      "Epoch 960/1000, Training Loss: 0.0258 Time Left: 1.24 seconds\n",
      "Epoch 961/1000, Training Loss: 0.0192 Time Left: 1.21 seconds\n",
      "Epoch 962/1000, Training Loss: 0.0228 Time Left: 1.17 seconds\n",
      "Epoch 963/1000, Training Loss: 0.0311 Time Left: 1.14 seconds\n",
      "Epoch 964/1000, Training Loss: 0.0170 Time Left: 1.11 seconds\n",
      "Epoch 965/1000, Training Loss: 0.0228 Time Left: 1.08 seconds\n",
      "Epoch 966/1000, Training Loss: 0.0344 Time Left: 1.05 seconds\n",
      "Epoch 967/1000, Training Loss: 0.0217 Time Left: 1.02 seconds\n",
      "Epoch 968/1000, Training Loss: 0.0239 Time Left: 0.99 seconds\n",
      "Epoch 969/1000, Training Loss: 0.0229 Time Left: 0.96 seconds\n",
      "Epoch 970/1000, Training Loss: 0.0232 Time Left: 0.93 seconds\n",
      "Epoch 971/1000, Training Loss: 0.0222 Time Left: 0.90 seconds\n",
      "Epoch 972/1000, Training Loss: 0.0253 Time Left: 0.87 seconds\n",
      "Epoch 973/1000, Training Loss: 0.0258 Time Left: 0.84 seconds\n",
      "Epoch 974/1000, Training Loss: 0.0250 Time Left: 0.80 seconds\n",
      "Epoch 975/1000, Training Loss: 0.0178 Time Left: 0.77 seconds\n",
      "Epoch 976/1000, Training Loss: 0.0272 Time Left: 0.74 seconds\n",
      "Epoch 977/1000, Training Loss: 0.0244 Time Left: 0.71 seconds\n",
      "Epoch 978/1000, Training Loss: 0.0288 Time Left: 0.68 seconds\n",
      "Epoch 979/1000, Training Loss: 0.0229 Time Left: 0.65 seconds\n",
      "Epoch 980/1000, Training Loss: 0.0252 Time Left: 0.62 seconds\n",
      "Epoch 981/1000, Training Loss: 0.0242 Time Left: 0.59 seconds\n",
      "Epoch 982/1000, Training Loss: 0.0198 Time Left: 0.56 seconds\n",
      "Epoch 983/1000, Training Loss: 0.0211 Time Left: 0.53 seconds\n",
      "Epoch 984/1000, Training Loss: 0.0252 Time Left: 0.50 seconds\n",
      "Epoch 985/1000, Training Loss: 0.0222 Time Left: 0.46 seconds\n",
      "Epoch 986/1000, Training Loss: 0.0229 Time Left: 0.43 seconds\n",
      "Epoch 987/1000, Training Loss: 0.0229 Time Left: 0.40 seconds\n",
      "Epoch 988/1000, Training Loss: 0.0275 Time Left: 0.37 seconds\n",
      "Epoch 989/1000, Training Loss: 0.0242 Time Left: 0.34 seconds\n",
      "Epoch 990/1000, Training Loss: 0.0271 Time Left: 0.31 seconds\n",
      "Epoch 991/1000, Training Loss: 0.0293 Time Left: 0.28 seconds\n",
      "Epoch 992/1000, Training Loss: 0.0201 Time Left: 0.25 seconds\n",
      "Epoch 993/1000, Training Loss: 0.0218 Time Left: 0.22 seconds\n",
      "Epoch 994/1000, Training Loss: 0.0266 Time Left: 0.19 seconds\n",
      "Epoch 995/1000, Training Loss: 0.0305 Time Left: 0.15 seconds\n",
      "Epoch 996/1000, Training Loss: 0.0255 Time Left: 0.12 seconds\n",
      "Epoch 997/1000, Training Loss: 0.0288 Time Left: 0.09 seconds\n",
      "Epoch 998/1000, Training Loss: 0.0325 Time Left: 0.06 seconds\n",
      "Epoch 999/1000, Training Loss: 0.0233 Time Left: 0.03 seconds\n",
      "Epoch 1000/1000, Validation Loss: 0.0603\n",
      "New best model found and saved with validation loss: 0.0603\n",
      "Epoch 1000/1000, Training Loss: 0.0195 Time Left: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_val_loss = math.inf  # Initialize best validation loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    for padded_encodings, encodings_mask, padded_query_indices, padded_query_targets, queries_mask in train_loader:\n",
    "        # Move data to GPU\n",
    "        padded_encodings = padded_encodings.to(device)\n",
    "        encodings_mask = encodings_mask.to(device)\n",
    "        padded_query_indices = padded_query_indices.to(device)\n",
    "        padded_query_targets = padded_query_targets.to(device)\n",
    "        queries_mask = queries_mask.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(padded_encodings, encodings_mask, padded_query_indices, queries_mask)\n",
    "        loss = model.loss(output, padded_query_targets, queries_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = epoch_train_loss / len(train_dataset)\n",
    "    \n",
    "    # Add validation every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for padded_encodings, encodings_mask, padded_query_indices, padded_query_targets, queries_mask in val_loader:\n",
    "                padded_encodings = padded_encodings.to(device)\n",
    "                encodings_mask = encodings_mask.to(device)\n",
    "                padded_query_indices = padded_query_indices.to(device)\n",
    "                padded_query_targets = padded_query_targets.to(device)\n",
    "                queries_mask = queries_mask.to(device)\n",
    "                output = model(padded_encodings, encodings_mask, padded_query_indices, queries_mask)\n",
    "                loss = model.loss(output, padded_query_targets, queries_mask)\n",
    "                epoch_val_loss += loss.item()\n",
    "        avg_val_loss = epoch_val_loss / len(val_dataset)\n",
    "        wandb.log({\"validation_loss\": avg_val_loss})\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"trained_model_best.pth\")\n",
    "            print(f\"New best model found and saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    estimated_total_time = elapsed_time / (epoch + 1) * num_epochs\n",
    "    estimated_time_left = estimated_total_time - elapsed_time\n",
    "    wandb.log({\"loss\": avg_train_loss})\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f} Time Left: {estimated_time_left:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the trained model\n",
    "wandb.finish()\n",
    "torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "print(\"Training complete. Model saved to trained_model.pth\")\n",
    "print(f\"Best validation loss: {best_val_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
